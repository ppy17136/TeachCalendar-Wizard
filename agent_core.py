import streamlit as st
import json
import time
from llm_wrapper import ai_generate
from skills import SyllabusSkills

class AgentCore:
    def __init__(self, keys_config, provider="Gemini", model_name="gemini-1.5-pro"):
        self.keys_config = keys_config
        self.provider = provider
        self.model_name = model_name
        self.skills = SyllabusSkills(keys_config, model_name)
        self.history = []
        
    def log(self, message):
        """Output to Streamlit UI or Console"""
        if "agent_logs" not in st.session_state:
            st.session_state.agent_logs = []
        st.session_state.agent_logs.append(message)
        # Using st.write directly might break if not in correct context, usually handled by caller
        
    def run_syllabus_generation(self, user_inputs, uploaded_texts):
        """
        Main Agent Loop for Syllabus Generation.
        user_inputs: dict of form fields (course_name, hours, etc.)
        uploaded_texts: dict of { "textbook": "...", "plan": "..." }
        """
        
        # 1. Planning Phase
        yield "ğŸ¤– Agent å¯åŠ¨: æ­£åœ¨é˜…è¯»ç”¨æˆ·éœ€æ±‚..."
        course_name = user_inputs.get('course_name')
        
        yield f"ğŸ“˜ æ­£åœ¨åˆ†ææ•™æ: {user_inputs.get('textbook_name', 'æœªå‘½å')}"
        textbook_content = uploaded_texts.get('textbook', '')
        # Simulate simple "Thought" - check if textbook is long
        if len(textbook_content) > 20000:
             yield "âš ï¸ æ•™æå†…å®¹è¿‡é•¿ï¼Œå¯åŠ¨æ™ºèƒ½åˆ‡ç‰‡é˜…è¯»æ¨¡å¼..."
             textbook_excerpt = textbook_content[:15000] # Simple truncate for now
        else:
             textbook_excerpt = textbook_content
             
        # 2. Validation Phase (Skill Use)
        yield "ğŸ” æ­£åœ¨è¿›è¡Œ OBE ç›®æ ‡æ ¡éªŒ..."
        obe_check = self.skills.validate_obe_compliance(user_inputs.get('objectives', ''))
        if not obe_check.get("is_compliant"):
            yield f"ğŸ’¡ å‘ç°ä¼˜åŒ–ç©ºé—´: {obe_check.get('analysis')}"
            # Auto-optimize logic could go here, for now we just log it
        
        # 3. Generation Phase
        yield "âœï¸ å¼€å§‹æ„æ€å¤§çº²ç»“æ„..."
        
        # Construct the mega-prompt (similar to original app but structured by Agent)
        # In a full Agent system, this would be broken down into steps like:
        # Step 1: Generate Basic Info Table
        # Step 2: Generate Chapter Allocation
        # Step 3: Refine
        
        # For stability, we keep the robust single-pass generation but wrap it in the Agent's persona
        final_prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æ•™å­¦è¾…åŠ© Agentã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯æ’°å†™ã€Š{course_name}ã€‹çš„æ•™å­¦å¤§çº²ã€‚
        
        [è¾“å…¥å‚æ•°]
        {json.dumps(user_inputs, ensure_ascii=False)}
        
        [æ•™ææ‘˜è¦]
        {textbook_excerpt}
        
        [åŸ¹å…»æ–¹æ¡ˆ]
        {uploaded_texts.get('plan', '')[:10000]}
        
        [OBE æ ¡éªŒåé¦ˆ]
        {json.dumps(obe_check, ensure_ascii=False)}
        
        è¯·ç”Ÿæˆå®Œæ•´å¤§çº²ï¼ŒMarkdown æ ¼å¼ã€‚
        """
        
        yield "ğŸš€ æ­£åœ¨ç”Ÿæˆæœ€ç»ˆå¤§çº²å†…å®¹ (è¿™å¯èƒ½éœ€è¦ 30 ç§’)..."
        result = ai_generate(final_prompt, self.provider, self.model_name, self.keys_config)
        
        # Yield the final result data wrapper
        yield {"final_result": result}
        yield "âœ… ç”Ÿæˆå®Œæˆï¼"

