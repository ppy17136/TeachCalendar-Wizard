import os
import streamlit as st
import pdfplumber
import fitz  # PyMuPDF
from docx import Document
import mammoth
import requests
import re
import numpy as np
import matplotlib.pyplot as plt
from openai import OpenAI
import base64
import io
import time
import hashlib
from dataclasses import dataclass
from pathlib import Path
from decimal import Decimal
import datetime as _dt
from PIL import Image
import google.generativeai as genai
import json
from docxtpl import DocxTemplate  # å¿…é¡»å®‰è£… docxtpl
from docx import Document
from docx.oxml import OxmlElement
from docx.oxml.ns import qn
from datetime import datetime
# ç­¾åæ’å…¥ç¤ºä¾‹
from docxtpl import InlineImage
from docx.shared import Mm, Pt
import pandas as pd  # å¿…é¡»æ·»åŠ ï¼Œç”¨äºæ•°æ®ç±»å‹æ¸…æ´—

# --- 1. åŸºç¡€ç¯å¢ƒä¸é…ç½® ---
plt.rcParams['font.family'] = ['SimHei', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False

# --- 2. çŠ¶æ€è‡ªåŠ¨åŒ–åˆå§‹åŒ– (åœ¨ app.py é¡¶éƒ¨) ---
if "calendar_data" not in st.session_state:
    st.session_state.calendar_data = [] # åˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨ï¼Œé˜²æ­¢ AttributeError
if "calendar_status" not in st.session_state:
    st.session_state.calendar_status = "Draft" # åˆå§‹çŠ¶æ€ä¸ºè‰æ‹Ÿ
if "calendar_final_data" not in st.session_state:
    st.session_state.calendar_final_data = None # æäº¤åçš„å®Œæ•´æ•°æ®åŒ…

st.set_page_config(page_title="æ™ºèƒ½æ•™å­¦è¾…åŠ©ç³»ç»Ÿ", layout="wide", initial_sidebar_state="expanded")

# --- çŠ¶æ€è‡ªåŠ¨åŒ–åˆå§‹åŒ– (é˜²æ­¢å˜é‡æœªå®šä¹‰æŠ¥é”™) ---
if "school_name" not in st.session_state:
    st.session_state.school_name = "è¾½å®çŸ³æ²¹åŒ–å·¥å¤§å­¦" # ç»™ä¸€ä¸ªåˆå§‹é»˜è®¤å€¼
    
# --- 3. å¯†é’¥è·å–ä¸ä¾§è¾¹æ  ---
BACKEND_QWEN_KEY = st.secrets.get("QWEN_API_KEY", "")
BACKEND_GEMINI_KEY = st.secrets.get("GEMINI_API_KEY", "")

# --- 2. çŠ¶æ€è‡ªåŠ¨åŒ–åˆå§‹åŒ– (é˜²æ­¢å˜é‡æœªå®šä¹‰æŠ¥é”™) ---
# åˆå§‹åŒ–å…¨å±€ä¼šè¯çŠ¶æ€
if "score_records" not in st.session_state:
    st.session_state.score_records = []
if "generated_syllabus" not in st.session_state:
    st.session_state.generated_syllabus = None
if "generated_calendar" not in st.session_state:
    st.session_state.generated_calendar = None
if "generated_program" not in st.session_state:
    st.session_state.generated_program = None
# ä½¿ç”¨ setdefault ç¡®ä¿å˜é‡ä¸€å®šå­˜åœ¨
st.session_state.setdefault("score_records", [])
st.session_state.setdefault("gen_content", {"syllabus": None, "calendar": None, "program": None})
# --- 3. ä¾§è¾¹æ ï¼šå¼•æ“åˆ‡æ¢ä¸å¯†é’¥ç®¡ç† ---
with st.sidebar:
    st.header("âš™ï¸ æ¨¡å‹å¼•æ“è®¾ç½®")
    selected_provider = st.radio("é€‰æ‹©ä¸» AI å¼•æ“", ["Gemini", "Qwen (é€šä¹‰åƒé—®)"])
    
    ACTIVE_QWEN_KEY = BACKEND_QWEN_KEY
    ACTIVE_GEMINI_KEY = BACKEND_GEMINI_KEY

    if selected_provider == "Gemini":
        user_gem_key = st.text_input("å¡«å†™ Gemini API Key (å¯é€‰)", type="password", help="ç•™ç©ºåˆ™ä½¿ç”¨åå°é»˜è®¤ Key")
        if user_gem_key: ACTIVE_GEMINI_KEY = user_gem_key
        selected_model = st.selectbox("ç‰ˆæœ¬", ["gemini-2.5-flash", "gemini-2.0-flash-exp", "gemini-2.5-pro"])
        engine_id = "Gemini"
        if ACTIVE_GEMINI_KEY: 
            genai.configure(api_key=ACTIVE_GEMINI_KEY)
        else:
            st.error("âš ï¸ æœªæ£€æµ‹åˆ°æœ‰æ•ˆ Gemini Key")
    else:
        user_qw_key = st.text_input("å¡«å†™ Qwen API Key (å¯é€‰)", type="password", help="ç•™ç©ºåˆ™ä½¿ç”¨åå°é»˜è®¤ Key")
        if user_qw_key: ACTIVE_QWEN_KEY = user_qw_key
        selected_model = st.selectbox("ç‰ˆæœ¬", ["qwen-plus", "qwen-max", "qwen-turbo"])
        engine_id = "Qwen"
        if not ACTIVE_QWEN_KEY:
            st.error("âš ï¸ æœªæ£€æµ‹åˆ°æœ‰æ•ˆ Qwen Key")

    st.divider()
    st.info(f"ğŸ’¡ å½“å‰æ¨¡å¼ï¼šä½¿ç”¨ **{engine_id}** å¤„ç†ã€‚")
    # ä¾§è¾¹æ åº•éƒ¨ä¹Ÿå¯ä»¥åŠ æç¤º
    st.caption("ğŸ–¥ï¸ å»ºè®®ç¯å¢ƒï¼šGoogle Chrome æµè§ˆå™¨")
    
# --- 4. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° --- 
def create_docx(text):
    """å°†æ–‡æœ¬è½¬æ¢ä¸ºå¯ä¸‹è½½çš„ Wordï¼Œå½»åº•æ¸…æ´—æ‰€æœ‰æ ‡è®°"""
    doc = Document()
    
    # 1. é¦–å…ˆé€šè¿‡æ­£åˆ™è¡¨è¾¾å¼æ¸…é™¤æ‰€æœ‰ HTML æ ‡ç­¾ (å¦‚ <br/>)
    # 2. æ¥ç€é€šè¿‡é“¾å¼ replace æ¸…é™¤ Markdown çš„æ ‡é¢˜å·å’ŒåŠ ç²—ç¬¦å·
    clean_text = re.sub('<[^<]+?>', '', text) \
                   .replace("### ", "") \
                   .replace("## ", "") \
                   .replace("# ", "") \
                   .replace("**", "")
    
    # å†™å…¥ Word
    for line in clean_text.split('\n'):
        if line.strip(): # è¿‡æ»¤æ‰å¤šä½™çš„ç©ºè¡Œ
            p = doc.add_paragraph(line)
            p.style.font.size = Pt(12)
    
    bio = io.BytesIO()
    doc.save(bio)
    return bio.getvalue()

def ai_generate(prompt, provider, model_name):
    """ç»Ÿä¸€æ–‡æœ¬ç”Ÿæˆæ¥å£"""
    if provider == "Gemini":
        if not ACTIVE_GEMINI_KEY: return "é”™è¯¯ï¼šæœªé…ç½®å¯†é’¥"
        try:
            model = genai.GenerativeModel(model_name)
            response = model.generate_content(prompt)
            return response.text
        except Exception as e: return f"Gemini å¤±è´¥: {str(e)}"
    else:
        if not ACTIVE_QWEN_KEY: return "é”™è¯¯ï¼šæœªé…ç½®å¯†é’¥"
        client = OpenAI(api_key=ACTIVE_QWEN_KEY, base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")
        try:
            completion = client.chat.completions.create(model=model_name, messages=[{"role": "user", "content": prompt}])
            return completion.choices[0].message.content
        except Exception as e: return f"Qwen å¤±è´¥: {str(e)}"

def ai_ocr(image_bytes, provider, model_name):
    """æ ¹æ®å¼•æ“è¿›è¡Œå›¾ç‰‡æ–‡å­—è¯†åˆ«"""
    if provider == "Gemini":
        if not ACTIVE_GEMINI_KEY: return "é”™è¯¯ï¼šæœªé…ç½®å¯†é’¥"
        try:
            model = genai.GenerativeModel(model_name)
            res = model.generate_content(["è¯†åˆ«å¹¶è¾“å‡ºå›¾ä¸­æ–‡å­—å†…å®¹ã€‚è‹¥æ˜¯è¯•å·ï¼Œè¯·æå–é¢˜ç›®å’Œå›ç­”ã€‚", {"mime_type": "image/jpeg", "data": image_bytes}])
            return res.text
        except Exception as e: return f"Gemini è§†è§‰è¯†åˆ«å¤±è´¥: {str(e)}"
    else:
        if not ACTIVE_QWEN_KEY: return "é”™è¯¯ï¼šæœªé…ç½®å¯†é’¥"
        # å›¾ç‰‡å‹ç¼©ä¼˜åŒ–
        img = Image.open(io.BytesIO(image_bytes)).convert('RGB')
        max_width = 1024
        if img.width > max_width:
            scale = max_width / img.width
            img = img.resize((max_width, int(img.height * scale)))
        buf = io.BytesIO()
        img.save(buf, format="JPEG", quality=90)
        b64img = base64.b64encode(buf.getvalue()).decode("utf-8")
        
        client = OpenAI(api_key=ACTIVE_QWEN_KEY, base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")
        try:
            completion = client.chat.completions.create(
                model="qwen-vl-ocr-latest",
                messages=[{"role": "user", "content": [{"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64img}"}}, {"type": "text", "text": "è¯·æå–å›¾ä¸­æ‰€æœ‰æ–‡å­—å†…å®¹"}]}]
            )
            return completion.choices[0].message.content
        except Exception as e: return f"Qwen OCR å¤±è´¥: {str(e)}"

# --- 5. æ–‡æ¡£ä¸å·¥å…·å‡½æ•° ---
def extract_text_from_file(file):
    """æ”¯æŒå¤šæ ¼å¼æ–‡æœ¬æå–"""
    try:
        if file.name.endswith(".docx"):
            return "\n".join([p.text for p in Document(file).paragraphs])
        elif file.name.endswith(".pdf"):
            with pdfplumber.open(file) as pdf:
                return "\n".join([page.extract_text() or "" for page in pdf.pages])
        elif file.name.endswith(".doc"):
            return mammoth.convert_to_text(file).value
        return "æ ¼å¼æš‚ä¸æ”¯æŒ"
    except Exception as e:
        return f"è§£æå¤±è´¥: {str(e)}"


def safe_extract_text(file, max_chars=15000):
    if not file: return ""
    try:
        text_list = []
        if file.name.endswith(".pdf"):
            with fitz.open(stream=file.read(), filetype="pdf") as doc:
                for page in doc:
                    text_list.append(page.get_text())
                    if sum(len(t) for t in text_list) > max_chars: break
            return "".join(text_list)[:max_chars]
            
        elif file.name.endswith(".docx"):
            doc = Document(file)
            for p in doc.paragraphs:
                if p.text.strip(): text_list.append(p.text)
            
            for table in doc.tables:
                for row in table.rows:
                    processed_cells = []
                    for cell in row.cells:
                        content = cell.text
                        # --- æ ¸å¿ƒæ”¹è¿›ï¼šéäº’æ–¥å…¨é‡æ›¿æ¢ï¼Œæ¶µç›–æ›´å¤š Word ç‰¹æ®Šç¬¦å· ---
                        # è¯†åˆ«â€œå·²é€‰ä¸­â€ç¬¦å·
                        checked_chars = ['â˜‘', 'Ã¾', '\xfe', '\uf0fe', 'â˜’', 'âˆš']
                        # è¯†åˆ«â€œæœªé€‰ä¸­â€ç¬¦å·
                        unchecked_chars = ['â˜', 'Â¨', '\xa8', '\uf0a1', 'â–¡']
                        
                        for c in checked_chars:
                            content = content.replace(c, '[å·²é€‰ä¸­]')
                        for u in unchecked_chars:
                            content = content.replace(u, '[æœªé€‰ä¸­]')
                        
                        processed_cells.append(content.strip())
                    
                    row_text = [c for c in processed_cells if c]
                    if row_text: text_list.append(" | ".join(row_text))
            
            return "\n".join(text_list)[:max_chars]
        elif file.name.endswith(".doc"):
            return mammoth.convert_to_text(file).value[:max_chars]            
        return ""

    except Exception as e:
        st.error(f"æ–‡ä»¶ {file.name} è§£æå‡ºé”™: {str(e)}")
        return ""

def render_pdf_images(pdf_file):
    images = []
    pdf_file.seek(0)
    with fitz.open(stream=pdf_file.read(), filetype="pdf") as pdf:
        for page in pdf:
            pix = page.get_pixmap(matrix=fitz.Matrix(2,2))
            images.append(pix.tobytes("png"))
    return images


# -----------------------------
# JSON å¯åºåˆ—åŒ–å·¥å…·ï¼ˆç”¨äºä¸‹è½½åŸºåº§/è°ƒè¯•ï¼‰
# -----------------------------
def payload_to_jsonable(obj):
    """é€’å½’æŠŠå¸¸è§ä¸å¯ JSON åºåˆ—åŒ–å¯¹è±¡è½¬æˆå¯åºåˆ—åŒ–ç»“æ„ã€‚"""
    # pandas
    try:
        if isinstance(obj, pd.DataFrame):
            df = obj.copy().fillna("")
            return {
                "__type__": "dataframe",
                "columns": [str(c) for c in df.columns.tolist()],
                "data": df.astype(str).values.tolist(),
            }
        if hasattr(pd, "Timestamp") and isinstance(obj, pd.Timestamp):
            return obj.isoformat()
    except Exception:
        pass

    # numpy
    try:
        import numpy as _np
        if isinstance(obj, (_np.integer, _np.floating, _np.bool_)):
            return obj.item()
        if isinstance(obj, _np.ndarray):
            return obj.tolist()
    except Exception:
        pass

    # bytes
    if isinstance(obj, (bytes, bytearray)):
        return {
            "__type__": "bytes_base64",
            "data": base64.b64encode(bytes(obj)).decode("ascii"),
        }

    # datetime / date
    if isinstance(obj, (_dt.datetime, _dt.date)):
        return obj.isoformat()

    # Path
    if isinstance(obj, Path):
        return str(obj)

    # Decimal
    if isinstance(obj, Decimal):
        return float(obj)

    # set/tuple
    if isinstance(obj, (set, tuple)):
        return [payload_to_jsonable(x) for x in obj]

    # dict / list
    if isinstance(obj, dict):
        return {str(k): payload_to_jsonable(v) for k, v in obj.items()}
    if isinstance(obj, list):
        return [payload_to_jsonable(x) for x in obj]

    # å…¶å®ƒï¼šå°½é‡åŸæ ·è¿”å›ï¼Œå¿…è¦æ—¶è½¬å­—ç¬¦ä¸²
    try:
        json.dumps(obj)
        return obj
    except Exception:
        return str(obj)


# -----------------------------
# åŸ¹å…»æ–¹æ¡ˆåŸºåº§ï¼šPDF æ–‡æœ¬ + é™„è¡¨ï¼ˆ7-10ï¼‰æŠ½å–ä¸è·¨é¡µåˆå¹¶
# -----------------------------
_SECTION_PATTERNS = [
    ("1", [r"ä¸€[ã€\.\s]*åŸ¹å…»ç›®æ ‡", r"1[ã€\.\s]*åŸ¹å…»ç›®æ ‡"]),
    ("2", [r"äºŒ[ã€\.\s]*æ¯•ä¸šè¦æ±‚", r"2[ã€\.\s]*æ¯•ä¸šè¦æ±‚"]),
    ("3", [r"ä¸‰[ã€\.\s]*ä¸“ä¸šå®šä½ä¸ç‰¹è‰²", r"3[ã€\.\s]*ä¸“ä¸šå®šä½ä¸ç‰¹è‰²"]),
    ("4", [r"å››[ã€\.\s]*ä¸»å¹²å­¦ç§‘", r"4[ã€\.\s]*ä¸»å¹²å­¦ç§‘"]),
    ("5", [r"äº”[ã€\.\s]*æ ‡å‡†å­¦åˆ¶", r"5[ã€\.\s]*æ ‡å‡†å­¦åˆ¶"]),
    ("6", [r"å…­[ã€\.\s]*æ¯•ä¸šæ¡ä»¶", r"6[ã€\.\s]*æ¯•ä¸šæ¡ä»¶"]),
    ("7", [r"ä¸ƒ[ã€\.\s]*ä¸“ä¸šæ•™å­¦è®¡åˆ’è¡¨", r"é™„è¡¨\s*1", r"7[ã€\.\s]*ä¸“ä¸šæ•™å­¦è®¡åˆ’è¡¨"]),
    ("8", [r"å…«[ã€\.\s]*å­¦åˆ†ç»Ÿè®¡è¡¨", r"é™„è¡¨\s*2", r"8[ã€\.\s]*å­¦åˆ†ç»Ÿè®¡è¡¨"]),
    ("9", [r"ä¹[ã€\.\s]*æ•™å­¦è¿›ç¨‹è¡¨", r"é™„è¡¨\s*3", r"9[ã€\.\s]*æ•™å­¦è¿›ç¨‹è¡¨"]),
    ("10", [r"å[ã€\.\s]*è¯¾ç¨‹è®¾ç½®å¯¹æ¯•ä¸šè¦æ±‚æ”¯æ’‘å…³ç³»è¡¨", r"é™„è¡¨\s*4", r"10[ã€\.\s]*è¯¾ç¨‹è®¾ç½®å¯¹æ¯•ä¸šè¦æ±‚"]),
    ("11", [r"åä¸€[ã€\.\s]*è¯¾ç¨‹è®¾ç½®é€»è¾‘æ€ç»´å¯¼å›¾", r"é™„è¡¨\s*5", r"11[ã€\.\s]*è¯¾ç¨‹è®¾ç½®é€»è¾‘æ€ç»´å¯¼å›¾"]),
]


def _compact_lines(s: str) -> str:
    s = (s or "").replace("\u00a0", " ")
    s = re.sub(r"[ \t]+", " ", s)
    s = re.sub(r"\n{3,}", "\n\n", s)
    return s.strip()


def _read_pdf_pages_text(pdf_bytes: bytes) -> list[str]:
    pages = []
    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
        for p in pdf.pages:
            txt = p.extract_text() or ""
            pages.append(_compact_lines(txt))
    return pages


def _join_pages(pages_text: list[str]) -> str:
    return _compact_lines("\n\n".join([t or "" for t in pages_text]))


def _build_section_spans(full_text: str) -> dict[str, tuple[int, int]]:
    hits = []
    for sec_id, pats in _SECTION_PATTERNS:
        pos = None
        for pat in pats:
            m = re.search(pat, full_text)
            if m:
                pos = m.start()
                break
        if pos is not None:
            hits.append((sec_id, pos))
    hits.sort(key=lambda x: x[1])
    spans = {}
    for i, (sec_id, start) in enumerate(hits):
        end = hits[i + 1][1] if i + 1 < len(hits) else len(full_text)
        spans[sec_id] = (start, end)
    return spans


def _extract_section_text(full_text: str, spans: dict[str, tuple[int, int]], sec_id: str) -> str:
    if sec_id not in spans:
        return ""
    s, e = spans[sec_id]
    chunk = (full_text[s:e] or "").strip()
    # å»æ‰æ ‡é¢˜è¡Œ
    chunk = re.sub(r"^\s*(ä¸€|äºŒ|ä¸‰|å››|äº”|å…­|ä¸ƒ|å…«|ä¹|å|åä¸€|\d+)[ã€\.\s]*[^\n]{0,60}\n", "", chunk)
    return _compact_lines(chunk)


def _valid_table_settings_lines() -> dict:
    return dict(
        vertical_strategy="lines",
        horizontal_strategy="lines",
        snap_tolerance=3,
        join_tolerance=3,
        edge_min_length=3,
        intersection_tolerance=3,
        text_tolerance=3,
    )


def _safe_text(x) -> str:
    return "" if x is None else str(x).strip()


def _table_to_df(table_rows: list[list[str]]) -> pd.DataFrame:
    rows = [r for r in table_rows if any(_safe_text(x) for x in r)]
    if not rows:
        return pd.DataFrame()
    max_cols = max(len(r) for r in rows)
    rows = [r + [""] * (max_cols - len(r)) for r in rows]

    header = rows[0]
    header_join = " ".join(header)
    header_like = any(k in header_join for k in ["è¯¾ç¨‹", "å­¦åˆ†", "å‘¨æ¬¡", "æŒ‡æ ‡", "æ”¯æ’‘", "åˆè®¡", "è¯¾ç¨‹ç¼–ç ", "è¯¾ç¨‹åç§°"])
    if header_like:
        cols = [c if c else f"åˆ—{i+1}" for i, c in enumerate(header)]
        cols = _dedup_cols([_safe_text(c) for c in cols])
        df = pd.DataFrame(rows[1:], columns=cols)
    else:
        df = pd.DataFrame(rows, columns=[f"åˆ—{i+1}" for i in range(max_cols)])

    return _clean_df(df)


def _dedup_cols(cols: list[str]) -> list[str]:
    seen = {}
    out = []
    for c in cols:
        c0 = c.strip() or "åˆ—"
        if c0 not in seen:
            seen[c0] = 1
            out.append(c0)
        else:
            seen[c0] += 1
            out.append(f"{c0}_{seen[c0]}")
    return out


def _clean_df(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty:
        return pd.DataFrame()
    df = df.copy()
    df.replace({None: ""}, inplace=True)
    df = df.applymap(lambda x: "" if str(x).strip().lower() == "nan" else str(x).strip())
    df = df.loc[~df.apply(lambda r: all(str(x).strip() == "" for x in r), axis=1)]
    df = df.loc[:, ~df.apply(lambda c: all(str(x).strip() == "" for x in c), axis=0)]
    return df.reset_index(drop=True)


def _header_similarity(cols_a: list[str], cols_b: list[str]) -> float:
    a = {re.sub(r"\s+", "", c.lower()) for c in cols_a if str(c).strip()}
    b = {re.sub(r"\s+", "", c.lower()) for c in cols_b if str(c).strip()}
    if not a or not b:
        return 0.0
    return len(a & b) / max(1, len(a | b))


def _classify_table(df: pd.DataFrame) -> tuple[str, int]:
    if df is None or df.empty:
        return ("", 0)
    s = (" ".join([str(c) for c in df.columns.tolist()]) + " " + " ".join(df.astype(str).head(3).values.flatten())).lower()

    def score(keys):
        return sum(3 for k in keys if k in s)

    score7 = score(["è¯¾ç¨‹ç¼–ç ", "è¯¾ç¨‹ä»£ç ", "è¯¾ç¨‹åç§°", "å­¦åˆ†", "æ€»å­¦æ—¶", "è€ƒæ ¸", "å¼€è¯¾"])
    score8 = score(["å­¦åˆ†ç»Ÿè®¡", "å¿…ä¿®", "é€‰ä¿®", "é€šè¯†", "ä¸“ä¸š", "å®è·µ", "åˆè®¡", "å°è®¡"])
    score9 = score(["å‘¨æ¬¡", "æ•™å­¦å†…å®¹", "è¿›åº¦", "ç« èŠ‚", "å­¦æ—¶", "å®éªŒ"])
    score10 = score(["æ¯•ä¸šè¦æ±‚", "æŒ‡æ ‡ç‚¹", "æ”¯æ’‘", "è¾¾æˆ", "å¯¹åº”", "æ”¯æ’‘å…³ç³»"])

    best = max([("7", score7), ("8", score8), ("9", score9), ("10", score10)], key=lambda x: x[1])
    return best if best[1] >= 6 else ("", 0)


def _extract_tables_with_meta(pdf_bytes: bytes, page_idx_list: list[int]) -> list[dict]:
    out = []
    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
        for pno in page_idx_list:
            if pno < 0 or pno >= len(pdf.pages):
                continue
            page = pdf.pages[pno]
            try:
                tables = page.extract_tables(table_settings=_valid_table_settings_lines()) or []
            except Exception:
                tables = page.extract_tables() or []
            for ti, t in enumerate(tables):
                norm = [[_safe_text(c) for c in row] for row in t]
                df = _table_to_df(norm)
                if df.empty:
                    continue
                out.append({"page": pno, "ti": ti, "df": df})
    return out


def _merge_tables_across_pages(items: list[dict]) -> pd.DataFrame:
    """æŠŠåŒä¸€é™„è¡¨åˆ†å¸ƒåœ¨å¤šé¡µçš„ df åˆå¹¶ï¼šæŒ‰é¡µæ’åºï¼Œè¡¨å¤´ç›¸ä¼¼åˆ™æ‹¼æ¥è¡Œã€‚"""
    if not items:
        return pd.DataFrame()
    items = sorted(items, key=lambda x: (x["page"], x["ti"]))
    base = items[0]["df"].copy()

    for it in items[1:]:
        df = it["df"].copy()
        sim = _header_similarity(base.columns.tolist(), df.columns.tolist())
        if sim < 0.45:
            # è¡¨å¤´å·®å¼‚å¤ªå¤§ï¼šä¸åˆå¹¶
            continue

        # å»æ‰é‡å¤è¡¨å¤´è¡Œï¼ˆå¸¸è§ï¼šç¬¬ä¸€é¡µè¡¨å¤´åœ¨æ¯é¡µé‡å¤å‡ºç°ï¼‰
        if len(df) >= 1:
            first_row = [str(x).strip() for x in df.iloc[0].tolist()]
            col_row = [str(x).strip() for x in df.columns.tolist()]
            if _header_similarity(first_row, col_row) > 0.7:
                df = df.iloc[1:].reset_index(drop=True)

        # ç»Ÿä¸€åˆ—
        all_cols = list(dict.fromkeys(list(base.columns) + list(df.columns)))
        base = base.reindex(columns=all_cols, fill_value="")
        df = df.reindex(columns=all_cols, fill_value="")
        base = pd.concat([base, df], ignore_index=True)

    return _clean_df(base)


def extract_appendix_tables_best_effort(pdf_bytes: bytes, pages_text: list[str]) -> tuple[dict[str, pd.DataFrame], dict]:
    n = len(pages_text)
    tail_pages = list(range(max(0, n - 24), n))  # æŠŠèŒƒå›´æ‰©å¤§ï¼Œè¦†ç›–â€œé™„è¡¨â€å¸¸è§ä½ç½®
    items = _extract_tables_with_meta(pdf_bytes, tail_pages)

    buckets: dict[str, list[dict]] = {"7": [], "8": [], "9": [], "10": []}
    scored_preview = []
    for it in items:
        sec, score = _classify_table(it["df"])
        if sec:
            buckets[sec].append({**it, "score": score})
            scored_preview.append((it["page"], it["ti"], sec, score, list(it["df"].shape)))

    assigned = {}
    for sec in ["7", "8", "9", "10"]:
        if not buckets[sec]:
            continue
        # åªç”¨å¾—åˆ†è¾ƒé«˜çš„ä¸€ç»„ï¼Œå¹¶å…è®¸è·¨é¡µåˆå¹¶
        buckets[sec].sort(key=lambda x: (x["score"], x["page"], x["ti"]), reverse=True)
        best_score = buckets[sec][0]["score"]
        group = [x for x in buckets[sec] if x["score"] >= max(6, best_score - 3)]
        assigned[sec] = _merge_tables_across_pages(group)

    debug = {
        "tail_pages": tail_pages,
        "tables_found": len(items),
        "scored_preview": scored_preview[:30],
        "assigned_shapes": {k: list(v.shape) for k, v in assigned.items()},
    }
    return assigned, debug


def base_plan_from_pdf(pdf_bytes: bytes) -> dict:
    pages = _read_pdf_pages_text(pdf_bytes)
    full = _join_pages(pages)
    spans = _build_section_spans(full)
    sections = {sec_id: _extract_section_text(full, spans, sec_id) for sec_id, _ in _SECTION_PATTERNS}

    # è‹¥ 7-11 åœ¨æ­£æ–‡åªæœ‰æ ‡é¢˜ï¼Œç»™æç¤ºï¼ˆä¸å¼ºè¡Œå¡å…¶ä»–å†…å®¹ï¼‰
    for sec_id in ["7", "8", "9", "10", "11"]:
        if not sections.get(sec_id, "").strip():
            sections[sec_id] = f"{sec_id}ï¼šæ­£æ–‡å¯èƒ½ä»…æœ‰æ ‡é¢˜ï¼›è¯·å°è¯•ä» PDF æœ«å°¾é™„è¡¨è‡ªåŠ¨æŠ½å–ã€‚"

    tables, debug = extract_appendix_tables_best_effort(pdf_bytes, pages)
    return {
        "pages": pages,
        "full_text": full,
        "sections": sections,
        "tables": tables,
        "debug": debug,
    }


# -----------------------------
# Word æ•™å­¦æ—¥å†ï¼šè‡ªåŠ¨â€œè½¬æ ‡ç­¾æ¨¡æ¿â€å·¥å…·ï¼ˆå…ˆä¸å¡«å……ï¼‰
# -----------------------------
_TAG_MAP = [
    (r"è¯¾ç¨‹åç§°", "course_name"),
    (r"è‹±æ–‡åç§°|è‹±æ–‡å", "english_name"),
    (r"è¯¾ç¨‹ä»£ç |è¯¾ç¨‹ç¼–ç ", "course_code"),
    (r"æ€»å­¦æ—¶|å­¦æ—¶", "hours"),
    (r"æ•™æ", "textbook"),
    (r"è€ƒæ ¸æ–¹å¼|è€ƒæ ¸", "assessment"),
]


def _replace_after_colon(text: str, field_key: str) -> str:
    # å½¢å¦‚ï¼šè¯¾ç¨‹åç§°ï¼šXXXX â†’ è¯¾ç¨‹åç§°ï¼š{{ course_name }}
    m = re.search(r"(:|ï¼š)\s*([^\n]+)", text)
    if not m:
        return text
    prefix = text[: m.start(2)]
    return prefix + f"{{{{ {field_key} }}}}"


def _tag_paragraph(p, pattern: str, key: str):
    if not p.text:
        return
    if re.search(pattern, p.text) and "{{" not in p.text:
        p.text = _replace_after_colon(p.text, key)


def _insert_row_before(table, row_idx: int):
    """python-docx åŸç”Ÿæ²¡æœ‰ insert_rowï¼Œä½¿ç”¨ oxml æ’å…¥ã€‚"""
    tbl = table._tbl
    tr = OxmlElement('w:tr')
    tbl.insert(row_idx, tr)
    # åˆ›å»ºä¸åˆ—æ•°ä¸€è‡´çš„å•å…ƒæ ¼
    for _ in range(len(table.columns)):
        tc = OxmlElement('w:tc')
        tcPr = OxmlElement('w:tcPr')
        tc.append(tcPr)
        p = OxmlElement('w:p')
        tc.append(p)
        tr.append(tc)
    return table.rows[row_idx]


def auto_tag_calendar_template(docx_bytes: bytes) -> bytes:
    doc = Document(io.BytesIO(docx_bytes))

    # 1) æ®µè½å­—æ®µæ›¿æ¢
    for p in doc.paragraphs:
        for pat, key in _TAG_MAP:
            _tag_paragraph(p, pat, key)

    # 2) è¡¨æ ¼å­—æ®µæ›¿æ¢ + è¯†åˆ«â€œæ—¥å†è¡¨â€å¹¶é‡å»ºä¸º for-loop ç»“æ„
    for t in doc.tables:
        # å…ˆå¯¹æ‰€æœ‰å•å…ƒæ ¼åšâ€œå­—æ®µï¼šå€¼â€æ›¿æ¢
        for row in t.rows:
            for cell in row.cells:
                for pat, key in _TAG_MAP:
                    if re.search(pat, cell.text) and "{{" not in cell.text:
                        cell.text = _replace_after_colon(cell.text, key)

        # å†åˆ¤æ–­æ˜¯å¦ä¸ºâ€œæ•™å­¦æ—¥å†è¡¨â€
        if len(t.rows) < 2:
            continue
        header = [c.text.strip() for c in t.rows[0].cells]
        header_join = "|".join(header)
        if ("å‘¨æ¬¡" in header_join) and ("è¯¾æ¬¡" in header_join) and (len(header) >= 6):
            # è‹¥å·²åŒ…å« for-loopï¼Œå°±ä¸é‡å¤å¤„ç†
            all_text = "\n".join([c.text for r in t.rows for c in r.cells])
            if "{% for" in all_text:
                continue

            # åˆ é™¤é™¤è¡¨å¤´å¤–çš„æ‰€æœ‰è¡Œ
            while len(t.rows) > 1:
                t._tbl.remove(t.rows[1]._tr)

            # æ’å…¥ for-row / data-row / endfor-row
            start_row = t.add_row()
            data_row = t.add_row()
            end_row = t.add_row()

            start_row.cells[0].text = "{% for s in calendar_table %}"
            end_row.cells[0].text = "{% endfor %}"

            # æ ¹æ®åˆ—åç»™é»˜è®¤å ä½
            col_keys = []
            for h in header:
                h2 = h.replace(" ", "")
                if "å‘¨æ¬¡" in h2:
                    col_keys.append("week")
                elif "è¯¾æ¬¡" in h2:
                    col_keys.append("session")
                elif "æ•™å­¦å†…å®¹" in h2:
                    col_keys.append("content")
                elif "å­¦ä¹ é‡ç‚¹" in h2 or "é‡ç‚¹" in h2:
                    col_keys.append("focus")
                elif "å­¦æ—¶" in h2:
                    col_keys.append("hours")
                elif "æ•™å­¦æ–¹æ³•" in h2 or "æ–¹æ³•" in h2:
                    col_keys.append("method")
                elif "æ”¯æ’‘" in h2:
                    col_keys.append("objective")
                else:
                    col_keys.append("col")

            for j, ck in enumerate(col_keys[: len(data_row.cells)]):
                if ck == "col":
                    data_row.cells[j].text = "{{ s.get('col', '') }}"
                else:
                    data_row.cells[j].text = f"{{{{ s.get('{ck}', '') }}}}"

            # ä¿æŒ start/end è¡Œå…¶å®ƒå•å…ƒæ ¼ä¸ºç©º
            for j in range(1, len(start_row.cells)):
                start_row.cells[j].text = ""
                end_row.cells[j].text = ""

    out = io.BytesIO()
    doc.save(out)
    return out.getvalue()

def nav_bar(show_back=False):
    st.markdown(f'<div style="background:#1E2129;padding:20px;border-radius:10px;margin-bottom:10px;"><h1 style="color:white;margin:0;font-size:24px;">ğŸ“ æ™ºèƒ½æ•™å­¦ä¸æ‰¹å·ç³»ç»Ÿ <span style="font-size:14px;color:#888;">{engine_id} å¼•æ“åœ¨çº¿</span></h1></div>', unsafe_allow_html=True)
    if show_back:
        if st.button("â¬…ï¸ è¿”å›ä¸»é¡µ", use_container_width=True):
            st.query_params["page"] = "é¦–é¡µ"
            st.rerun()

# --- 6. é¡µé¢åŠŸèƒ½å®šä¹‰ ---
def page_home():
    nav_bar()
    st.markdown("### ğŸ› ï¸ æ•™åŠ¡ä¸æ‰¹æ”¹åŠŸèƒ½çŸ©é˜µ")
    cols = st.columns(3)
    modules = [
        ("ğŸ“„", "æ•™å­¦å¤§çº²ç”Ÿæˆ", "å¤§çº²"), ("ğŸ“…", "æ•™å­¦æ—¥å†ç”Ÿæˆ", "æ—¥å†"), ("ğŸ“‹", "åŸ¹å…»æ–¹æ¡ˆç”Ÿæˆ", "æ–¹æ¡ˆ"),
        ("ğŸ—ï¸", "åŸ¹å…»æ–¹æ¡ˆåŸºåº§æŠ½å–", "åŸºåº§"), ("ğŸ·ï¸", "Word æ¨¡æ¿è½¬æ ‡ç­¾", "æ¨¡æ¿"),
        ("ğŸ“", "æ™ºèƒ½æ‰¹å·ç³»ç»Ÿ", "æ‰¹å·"), ("ğŸ“ˆ", "æˆç»©åˆ†ææŠ¥å‘Š", "åˆ†æ"), ("âš™ï¸", "ç³»ç»Ÿè®¾ç½®", "è®¾ç½®")
    ]
    for i, (icon, title, link) in enumerate(modules):
        with cols[i % 3]:
            st.markdown(f'<div style="border:1px solid #ddd;padding:20px;border-radius:10px;text-align:center;"><span style="font-size:40px;">{icon}</span><h4>{title}</h4></div>', unsafe_allow_html=True)
            if st.button(f"è¿›å…¥{title}", key=f"nav_{i}", use_container_width=True):
                st.query_params["page"] = link
                st.rerun()


def page_base_plan():
    """åŸ¹å…»æ–¹æ¡ˆ PDF â†’ 1â€“11 æ ç›®æŠ½å– + æœ«å°¾é™„è¡¨(7â€“10)è·¨é¡µåˆå¹¶ã€‚"""
    nav_bar(show_back=True)
    st.subheader("ğŸ—ï¸ åŸ¹å…»æ–¹æ¡ˆåŸºåº§æŠ½å–ï¼ˆ1â€“11 + é™„è¡¨ 7â€“10ï¼‰")
    st.caption("ä¸Šä¼ åŸ¹å…»æ–¹æ¡ˆ PDF â†’ è‡ªåŠ¨æŠ½å–æ­£æ–‡æ ç›®ä¸æœ«å°¾é™„è¡¨ï¼›é™„è¡¨ 1/4 è¿™ç±»è·¨å¤šé¡µçš„è¡¨ä¼šè‡ªåŠ¨åˆå¹¶ã€‚")

    pdf = st.file_uploader("ä¸Šä¼ åŸ¹å…»æ–¹æ¡ˆ PDF", type=["pdf"], key="base_plan_pdf")
    c1, c2 = st.columns([1, 1])
    with c1:
        tail_pages = st.number_input("é™„è¡¨æŠ½å–ï¼šä»æœ«å°¾å‘å‰å–é¡µæ•°", min_value=5, max_value=60, value=20, step=1)
    with c2:
        min_score = st.number_input("è¡¨æ ¼åˆ†ç±»é˜ˆå€¼ï¼ˆè¶Šå¤§è¶Šä¿å®ˆï¼‰", min_value=3, max_value=15, value=6, step=1)

    if st.button("å¼€å§‹æŠ½å–å¹¶å†™å…¥åŸºåº§", type="primary", use_container_width=True, key="base_plan_extract"):
        if not pdf:
            st.error("è¯·å…ˆä¸Šä¼  PDFã€‚")
        else:
            with st.spinner("æ­£åœ¨è§£æ PDF å¹¶æŠ½å–..."):
                payload = base_plan_from_pdf(pdf.getvalue(), tail_pages=int(tail_pages), min_score=int(min_score))
                st.session_state["base_plan_payload"] = payload
            st.success("æŠ½å–å®Œæˆã€‚ä¸‹æ–¹å¯æŸ¥çœ‹/ä¸‹è½½ã€‚")

    payload = st.session_state.get("base_plan_payload")
    if not payload:
        st.info("è¯·ä¸Šä¼  PDF å¹¶ç‚¹å‡»â€œå¼€å§‹æŠ½å–å¹¶å†™å…¥åŸºåº§â€ã€‚")
        return

    # ä¸‹è½½ JSON
    json_payload = payload_to_jsonable(payload)
    st.download_button(
        "ä¸‹è½½åŸºåº§ JSONï¼ˆå«æ ç›®æ–‡æœ¬+é™„è¡¨è¡¨æ ¼ï¼‰",
        data=json.dumps(json_payload, ensure_ascii=False, indent=2).encode("utf-8"),
        file_name="base_plan_payload.json",
        mime="application/json",
        use_container_width=True,
        key="dl_base_plan_json",
    )

    tabs = st.tabs(["æ ç›® 1â€“11", "é™„è¡¨ 7â€“10ï¼ˆå¯ç¼–è¾‘ï¼‰", "è°ƒè¯•ä¿¡æ¯"])
    with tabs[0]:
        toc = [
            ("1", "åŸ¹å…»ç›®æ ‡"), ("2", "æ¯•ä¸šè¦æ±‚"), ("3", "ä¸“ä¸šå®šä½ä¸ç‰¹è‰²"), ("4", "ä¸»å¹²å­¦ç§‘/æ ¸å¿ƒè¯¾ç¨‹/å®è·µç¯èŠ‚"),
            ("5", "æ ‡å‡†å­¦åˆ¶ä¸æˆäºˆå­¦ä½"), ("6", "æ¯•ä¸šæ¡ä»¶"), ("7", "ä¸“ä¸šæ•™å­¦è®¡åˆ’è¡¨ï¼ˆé™„è¡¨1ï¼‰"),
            ("8", "å­¦åˆ†ç»Ÿè®¡è¡¨ï¼ˆé™„è¡¨2ï¼‰"), ("9", "æ•™å­¦è¿›ç¨‹è¡¨ï¼ˆé™„è¡¨3ï¼‰"), ("10", "æ”¯æ’‘å…³ç³»è¡¨ï¼ˆé™„è¡¨4ï¼‰"),
            ("11", "é€»è¾‘æ€ç»´å¯¼å›¾ï¼ˆé™„è¡¨5ï¼‰"),
        ]
        sec_pick = st.selectbox("é€‰æ‹©æ ç›®", options=[x[0] for x in toc], format_func=lambda x: dict(toc)[x], key="base_plan_sec")
        st.markdown(f"#### {sec_pick}ã€{dict(toc)[sec_pick]}")
        st.text_area("æŠ½å–æ–‡æœ¬", value=payload.get("sections", {}).get(sec_pick, ""), height=280, key=f"base_plan_text_{sec_pick}")

    with tabs[1]:
        st.info("æç¤ºï¼šè¿™é‡Œå±•ç¤ºçš„æ˜¯è‡ªåŠ¨æŠ½å–å¹¶è·¨é¡µåˆå¹¶åçš„è¡¨ï¼›ä½ å¯ä»¥ç›´æ¥ç¼–è¾‘åå¯¼å‡ºã€‚")
        for sec in ["7", "8", "9", "10"]:
            st.markdown(f"#### é™„è¡¨ {sec}")
            df0 = payload.get("tables", {}).get(sec)
            if df0 is None or (isinstance(df0, pd.DataFrame) and df0.empty):
                st.warning("æœªæŠ½å–åˆ°è¯¥è¡¨ï¼ˆå¯èƒ½æ˜¯å›¾ç‰‡è¡¨ã€çº¿æ¡ä¸è§„åˆ™æˆ–ç‰ˆå¼ç‰¹æ®Šï¼‰ã€‚")
                df0 = pd.DataFrame()
            editor_key = f"base_tbl_{sec}"
            edited = st.data_editor(df0, num_rows="dynamic", use_container_width=True, key=editor_key)
            st.session_state[f"{editor_key}__value"] = edited

    with tabs[2]:
        st.json(payload.get("debug", {}))


def page_template_tagger():
    """æŠŠç”¨æˆ·ä¸Šä¼ çš„ docxï¼ˆæ™®é€šèŒƒæœ¬ï¼‰è½¬æ¢æˆå¯ç”¨ docxtpl çš„â€œå¸¦æ ‡ç­¾æ¨¡æ¿â€ï¼Œå¹¶æä¾›ä¸‹è½½ã€‚"""
    nav_bar(show_back=True)
    st.subheader("ğŸ·ï¸ Word èŒƒæœ¬ â†’ å¸¦æ ‡ç­¾æ¨¡æ¿ï¼ˆä»…è½¬æ¢ï¼Œä¸å¡«å……ï¼‰")
    st.caption("æŠŠâ€œè¯¾ç¨‹åç§°/è¯¾ç¨‹ä»£ç /å­¦æ—¶/æ•™æ/è€ƒæ ¸â€ç­‰å­—æ®µè‡ªåŠ¨æ›¿æ¢æˆ {{ æ ‡ç­¾ }}ï¼Œå¹¶æŠŠâ€˜æ•™å­¦æ—¥å†è¡¨â€™æ”¹æˆå¯å¾ªç¯çš„ {% for %} æ¨¡æ¿ç»“æ„ã€‚")

    up = st.file_uploader("ä¸Šä¼  Word èŒƒæœ¬ï¼ˆ.docxï¼‰", type=["docx"], key="tpl_tag_in")
    col1, col2 = st.columns([1, 1])
    with col1:
        loop_var = st.text_input("å¾ªç¯å˜é‡å", value="s", help="ç”¨äº calendar_table å¾ªç¯ï¼Œä¾‹å¦‚ {% for s in calendar_table %}")
    with col2:
        strict_mode = st.checkbox("ä¸¥æ ¼æ¨¡å¼ï¼ˆåªæ›¿æ¢å·²è¯†åˆ«å­—æ®µï¼‰", value=True)

    if st.button("è½¬æ¢ä¸ºå¸¦æ ‡ç­¾æ¨¡æ¿å¹¶ç”Ÿæˆä¸‹è½½", type="primary", use_container_width=True, key="tpl_tag_btn"):
        if not up:
            st.error("è¯·å…ˆä¸Šä¼  .docx èŒƒæœ¬ã€‚")
        else:
            with st.spinner("æ­£åœ¨è½¬æ¢..."):
                tagged_bytes, report = auto_tag_calendar_template(up.getvalue(), loop_var=loop_var.strip() or "s", strict=strict_mode)
                st.session_state["tagged_tpl_bytes"] = tagged_bytes
                st.session_state["tagged_tpl_report"] = report
            st.success("è½¬æ¢å®Œæˆã€‚è¯·ä¸‹è½½å¹¶äººå·¥å¿«é€Ÿæ£€æŸ¥ï¼šå°é¢å­—æ®µæ˜¯å¦è¢«æ­£ç¡®æ›¿æ¢ã€æ•™å­¦æ—¥å†è¡¨æ˜¯å¦åªä¿ç•™ä¸€è¡Œå ä½+å¾ªç¯æ ‡ç­¾ã€‚")

    tagged = st.session_state.get("tagged_tpl_bytes")
    if tagged:
        st.download_button(
            "ä¸‹è½½å¸¦æ ‡ç­¾æ¨¡æ¿ï¼ˆ.docxï¼‰",
            data=tagged,
            file_name="calendar_template_tagged.docx",
            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
            use_container_width=True,
            key="tpl_tag_dl",
        )
        with st.expander("è½¬æ¢æŠ¥å‘Šï¼ˆæ–¹ä¾¿æ’æŸ¥ï¼‰"):
            st.json(st.session_state.get("tagged_tpl_report", {}))

def page_syllabus():
    nav_bar(show_back=True)
    st.subheader("ğŸ“„ æ·±åº¦æ™ºé€ ï¼šæ•™å­¦å¤§çº² (æ”¯æŒä¸Šä¼ æ•™æåˆ†æ)")
    
    # 5.1 ä¸Šä¼ è¾…åŠ©èµ„æ–™åŒºåŸŸ
    with st.expander("##### ğŸ“š ç¬¬ä¸€æ­¥ï¼šä¸Šä¼ å‚è€ƒèµ„æ–™ (æ•™æ/åŸ¹å…»æ–¹æ¡ˆ/å‚è€ƒæ–‡çŒ®)", expanded=True):
        col_u1, col_u2 = st.columns(2)
        book_file = col_u1.file_uploader("ä¸Šä¼ æ•™æ/å‚è€ƒä¹¦ PDF/Word", type=["pdf", "docx"])
        plan_file = col_u2.file_uploader("ä¸Šä¼ äººæ‰åŸ¹å…»æ–¹æ¡ˆ PDF/Word", type=["pdf", "docx"])
        
    # 5.2 æ‰‹å·¥å¡«å†™åŸºæœ¬ä¿¡æ¯
    with st.form("syllabus_form"):
        st.markdown("##### ğŸ“š ç¬¬äºŒæ­¥ï¼šå¡«å†™å…³é”®å‚æ•°")        
        # ç¬¬ä¸€æ’ï¼šåŸºç¡€è¯¾ç¨‹ä¿¡æ¯ 
        c1, c2, c3 = st.columns(3)
        name = c1.text_input("è¯¾ç¨‹åç§°", value="æ•°å€¼æ¨¡æ‹Ÿåœ¨ææ–™æˆå‹ä¸­çš„åº”ç”¨")
        major = c2.text_input("é€‚ç”¨ä¸“ä¸š", value="ææ–™æˆå‹åŠæ§åˆ¶å·¥ç¨‹ï¼ˆç„Šæ¥æ–¹å‘ï¼‰")
        course_type = c3.selectbox("è¯¾ç¨‹æ€§è´¨", ["å¿…ä¿®", "é™é€‰", "é€‰ä¿®"], index=1)

        # ç¬¬äºŒæ’ï¼šå­¦åˆ†å­¦æ—¶ä¸è€ƒæ ¸ 
        c4, c5, c6 = st.columns(3)
        hours = c4.number_input("æ€»å­¦æ—¶", value=32)
        credits = c5.number_input("æ€»å­¦åˆ†", value=2.0, step=0.5)
        assessment = c6.selectbox("è€ƒæ ¸æ–¹å¼", ["è€ƒè¯•", "è€ƒæŸ¥"], index=1)

        # ç¬¬ä¸‰æ’ï¼šå­¦æœŸä¸è¦æ±‚ 
        c7, c8 = st.columns(2)
        semester = c7.selectbox("å¼€è¯¾å­¦æœŸ", ["ä¸€", "äºŒ", "ä¸‰", "å››", "äº”", "å…­", "ä¸ƒ", "å…«"], index=4)
        prerequisites = c8.text_area("å…ˆä¿®è¯¾ç¨‹è¦æ±‚", value="é«˜ç­‰æ•°å­¦ã€å·¥ç¨‹åŠ›å­¦ï¼Œå…·å¤‡åŸºæœ¬å¾®ç§¯åˆ†å’Œå·¥ç¨‹åŠ›å­¦çŸ¥è¯†", height=68)

        # æ ¸å¿ƒç›®æ ‡ä¸æ€æ”¿
        obj = st.text_area("åŸ¹å…»ç›®æ ‡", placeholder="è¾“å…¥è¯¾ç¨‹åŸ¹å…»ç›®æ ‡...", value="è¯¾ç¨‹ç›®æ ‡1ï¼šèƒ½å¤Ÿäº†è§£ææ–™æˆå‹çš„æ•°å€¼æ¨¡æ‹Ÿè½¯ä»¶çš„åŸç†å’Œæ–¹æ³•ï¼Œå¹¶ç†è§£å…¶å±€é™æ€§ï¼›\nè¯¾ç¨‹ç›®æ ‡2ï¼šèƒ½å¤Ÿé€‰ç”¨åˆé€‚çš„ä¸“ä¸šæ•°å€¼æ¨¡æ‹Ÿè½¯ä»¶åˆ†æææ–™æˆå‹å·¥ç¨‹ä¸­çš„å¤æ‚é—®é¢˜ï¼›\nè¯¾ç¨‹ç›®æ ‡3ï¼šèƒ½å¤Ÿé€‰ç”¨é€‚åˆçš„æ•°å€¼æ¨¡æ‹Ÿè½¯ä»¶é¢„æµ‹ææ–™æˆå‹å·¥ç¨‹é—®é¢˜ï¼Œå¹¶åˆ†æå…¶å±€é™æ€§ã€‚")
        ideology = st.text_area("æ€æ”¿èå…¥ç‚¹", value="å›½äº§å·¥ä¸šè½¯ä»¶å‘å±•ã€ä¸¤å¼¹ä¸€æ˜Ÿç²¾ç¥")

        if st.form_submit_button("ğŸš€ ç»“åˆä¸Šä¼ èµ„æ–™ç”Ÿæˆ OBE æ ‡å‡†å¤§çº²"):
            with st.spinner("æ­£åœ¨é˜…è¯»æ–‡æ¡£å¹¶æ„æ€å¤§çº²..."):
                #book_ctx = extract_text_from_file(book_file) if book_file else "æœªæä¾›æ•™æ"
                plan_ctx = extract_text_from_file(plan_file) if plan_file else "æœªæä¾›åŸ¹å…»æ–¹æ¡ˆ"   
                book_ctx = safe_extract_text(book_file) if book_file else "æœªæä¾›æ•™æ"
                #plan_ctx = safe_extract_text(plan_file) if plan_file else "æœªæä¾›åŸ¹å…»æ–¹æ¡ˆ"
                
                prompt = f"""
                        ä½ æ˜¯ä¸€ä½èµ„æ·±çš„é«˜æ ¡å·¥ç¨‹æ•™è‚²è®¤è¯ä¸“å®¶ã€‚è¯·ä¸ºã€Š{name}ã€‹è¯¾ç¨‹æ’°å†™ä¸€ä»½é«˜è´¨é‡æ•™å­¦å¤§çº²ã€‚æ–‡å­—ä¸“ä¸šä¸”ç¬¦åˆOBEç†å¿µã€‚
                        
                        **ä¸¥æ ¼æ’ç‰ˆè¦æ±‚ï¼š**
                        1. ç¦æ­¢ä½¿ç”¨ä»»ä½• HTML æ ‡ç­¾ï¼ˆå¦‚ <br/>, <b> ç­‰ï¼‰ã€‚
                        2. æ‰€æœ‰çš„è¡¨æ ¼å¿…é¡»ä½¿ç”¨æ ‡å‡† Markdown æ ¼å¼ï¼š| åˆ—1 | åˆ—2 |ã€‚
                        3. å¿…é¡»åŒ…å«åˆ†éš”çº¿ï¼š| :--- | :--- |ã€‚
                        4. æ¯ä¸ªè¡¨æ ¼ä¸Šæ–¹å’Œä¸‹æ–¹å¿…é¡»å„ç•™ä¸€è¡Œç©ºè¡Œã€‚
                        
                        **èƒŒæ™¯èµ„æ–™ï¼ˆè¯·åŠ¡å¿…å‚è€ƒä»¥ä¸‹å†…å®¹ï¼‰ï¼š**
                        1. æ•™æ/å†…å®¹æ ¸å¿ƒï¼š{book_ctx[:12000]} (æ³¨ï¼šç”±äºé•¿åº¦é™åˆ¶ï¼Œå·²æˆªå–å‰1ä¸‡å­—ç¬¦)
                        2. ä¸“ä¸šåŸ¹å…»è¦æ±‚ï¼š{plan_ctx[:10000]}
                        
                        **æ‰‹å·¥å¡«å†™çš„å‚æ•°ï¼š**                    
                        - è¯¾ç¨‹æ€§è´¨ï¼š{course_type} | è€ƒæ ¸æ–¹å¼ï¼š{assessment} | å­¦åˆ†ï¼š{credits} | å­¦æ—¶ï¼š{hours}
                        - é€‚ç”¨ä¸“ä¸šï¼š{major} | æ€æ”¿ï¼š{ideology} | å¼€è¯¾å­¦æœŸ{semester} | å…ˆä¿®è¯¾ç¨‹åŠå…¶è¦æ±‚{prerequisites}                   
                        - è¯¾ç¨‹ç›®æ ‡æ”¯æ’‘æ¯•ä¸šè¦æ±‚è¡¨ï¼ˆå«è¯¾ç¨‹ç›®æ ‡{obj}
                        
                        **å¤§çº²å¿…é¡»åŒ…å«ï¼š**
                        - è¯¾ç¨‹åŸºæœ¬ä¿¡æ¯è¡¨ï¼ŒåŒ…å«å¤§çº²åç§°ã€è¯¾ç¨‹åç§°{name}ã€è‹±æ–‡åç§°ã€ç¼–ç ã€è¯¾ç¨‹æ€§è´¨{course_type}ã€é€‚ç”¨ä¸“ä¸š{major}ã€è€ƒæ ¸æ–¹å¼{assessment}ã€æ€»å­¦åˆ†{credits}ã€æ€» å­¦ æ—¶{hours}ï¼ˆç†è®ºå­¦æ—¶Xã€å®éªŒå­¦æ—¶Xã€å®è®­å­¦æ—¶Xã€å…¶ä»–ï¼ˆè®¨è®ºï¼‰	å­¦æ—¶Xï¼‰ã€å¼€è¯¾å­¦æœŸ{semester}ã€å…ˆä¿®è¯¾ç¨‹åŠå…¶è¦æ±‚{prerequisites}ç­‰
                        - è¯¾ç¨‹ç®€ä»‹ï¼ˆç†å®ç»“åˆï¼Œä¸å°‘äº200å­—ï¼‰
                        - å»ºè®®æ•™æ	 
                        - å‚è€ƒèµ„æ–™	 
                        - æ•™å­¦æ¡ä»¶
                        - è¯¾ç¨‹ç›®æ ‡æ”¯æ’‘æ¯•ä¸šè¦æ±‚è¡¨ï¼ˆå«è¯¾ç¨‹ç›®æ ‡{obj}ã€æ”¯æ’‘æŒ‡æ ‡ç‚¹å¦‚4.1/5.1åŠæ”¯æ’‘å¼ºåº¦H/M/Lï¼‰
                        - å¾·è‚²ç›®æ ‡
                        - æ•™å­¦å†…å®¹å­¦æ—¶åˆ†é…è¡¨ï¼ˆç¡®ä¿æ€»å­¦æ—¶ä¸º{hours}ï¼‰ï¼ˆæ•™å­¦å†…å®¹å‚è€ƒæ•™æå’Œå‚è€ƒææ–™{book_ctx}ï¼ŒåŒ…å«åºå·ã€æ•™å­¦å†…å®¹ã€å­¦ç”Ÿå­¦ä¹ é¢„æœŸæˆæœã€è®¡åˆ’å­¦æ—¶ã€æ”¯æ’‘ç›®æ ‡ã€æ•™å­¦æ–¹å¼ã€å…¶å®ƒï¼ˆä½œä¸šã€ä¹ é¢˜ã€å®éªŒç­‰ï¼‰
                        - è¯¾ç¨‹ç›®æ ‡è€ƒæ ¸
                        - è¯¾ç¨‹ç›®æ ‡è¾¾æˆæƒ…å†µè¯„ä»·
                        - è€ƒæ ¸è¯„ä»·è¡¨ï¼ˆåŒ…å«å¹³æ—¶æˆç»©ä¸æœŸæœ«è€ƒè¯•å æ¯”ï¼‰                    
                        - è¯¾ç¨‹è€ƒæ ¸ï¼ŒåŒ…å«æ ‡å‡†è€ƒè¯•è¯„åˆ†æ ‡å‡†ã€ä½œä¸šè¯„åˆ†æ ‡å‡†
                        - å¤§ä½œä¸šè¯„åˆ†æ ‡å‡†ï¼ŒåŒ…å«ä½œä¸šå†…å®¹ã€è¯„ä»·æ ‡å‡†ï¼ˆ90-100åˆ†	70-89 åˆ†	60-69åˆ†	0-59åˆ†ï¼‰ã€æ‰€å æ¯”é‡
                        - è¯¾ç¨‹æ€æ”¿å®æ–½æ–¹æ¡ˆï¼ˆç»“åˆï¼š{ideology}ï¼‰ï¼ŒåŒ…å«æ€æ”¿å†…å®¹åˆ‡å…¥ç‚¹ã€å…¸å‹æ¡ˆä¾‹ã€æ•™è‚²è½½ä½“åŠæ–¹æ³•ã€é¢„æœŸè¾¾åˆ°çš„ç›®æ ‡ã€	ä½“ç°çš„ä»·å€¼è§‚æˆ–æ€æ”¿å…ƒç´ 
                        
                        **å°¤å…¶æ³¨æ„æ„å»ºã€Šè¯¾ç¨‹ç›®æ ‡æ”¯æ’‘æ¯•ä¸šè¦æ±‚è¡¨ã€‹æ—¶ï¼š**
                        è¯·åŸºäºåŸ¹å…»æ–¹æ¡ˆ{plan_ctx}ä¸¥æ ¼ä»¥ä¸‹å¯¹åº”å…³ç³»ç”Ÿæˆè¡¨æ ¼ï¼Œç¦æ­¢éšæ„å‘æŒ¥ï¼š
                        1. è¯¾ç¨‹ç›®æ ‡1ï¼š{obj.split('è¯¾ç¨‹ç›®æ ‡2')[0] if 'è¯¾ç¨‹ç›®æ ‡2' in obj else obj} 
                           --> å¿…é¡»æ”¯æ’‘ï¼š5.1 (å·¥å…·ä½¿ç”¨)ã€‚
                        2. è¯¾ç¨‹ç›®æ ‡2ï¼š... (ä»¥æ­¤ç±»æ¨ï¼Œè¯·è§£æç”¨æˆ·è¾“å…¥çš„ {obj})

                        **è¡¨æ ¼æ ¼å¼è¦æ±‚ï¼š**
                        | è¯¾ç¨‹ç›®æ ‡ | æ”¯æ’‘æ¯•ä¸šè¦æ±‚åŠæŒ‡æ ‡ç‚¹ | æ”¯æ’‘å¼ºåº¦ (H/M/L) |
                        | :--- | :--- | :--- |
                        | è¯¾ç¨‹ç›®æ ‡1ï¼š[ç®€è¿°ç›®æ ‡å†…å®¹] | 5.1 äº†è§£å¸¸ç”¨ç°ä»£ä»ªå™¨... | H |
                        | è¯¾ç¨‹ç›®æ ‡2ï¼š[ç®€è¿°ç›®æ ‡å†…å®¹] | 5.2 èƒ½å¤Ÿé€‰æ‹©ä¸ä½¿ç”¨æ°å½“ä»ªå™¨... | M |

                        **ç‰¹åˆ«æ³¨æ„ï¼š**
                        - æ¯ä¸€è¡Œåªèƒ½å¯¹åº”ä¸€ä¸ªè¯¾ç¨‹ç›®æ ‡ã€‚
                        - æ¯ä¸€ä¸ªè¯¾ç¨‹ç›®æ ‡åªèƒ½å¯¹åº”ä¸€ä¸ªæ¯•ä¸šè¦æ±‚åŠæŒ‡æ ‡ç‚¹
                        - æŒ‡æ ‡ç‚¹æè¿°å¿…é¡»å®Œæ•´ã€‚
                        - æ”¯æ’‘å¼ºåº¦å¿…é¡»æ ¹æ®è¯¥ç›®æ ‡å¯¹æŒ‡æ ‡ç‚¹çš„æ”¯æ’‘åŠ›åº¦ç»™å‡ºå”¯ä¸€çš„ Hã€M æˆ– Lã€‚                        
                        """            
                # æ‰§è¡Œç”Ÿæˆå¹¶å­˜å…¥ç¼“å­˜
                st.session_state.gen_content["syllabus"] = ai_generate(prompt, engine_id, selected_model)
                st.session_state['course_name'] = name
                st.session_state['total_hours'] = hours
                st.session_state['major'] = major # é€‚ç”¨ä¸“ä¸š
                #st.session_state['assessment_method'] = assessment # è€ƒæ ¸æ–¹å¼
                st.session_state['course_objectives'] = obj # å­˜å‚¨åŸå§‹è¾“å…¥çš„è¯¾ç¨‹ç›®æ ‡æ–‡æœ¬
                st.session_state['ideology_points'] = ideology # å­˜å‚¨æ€æ”¿ç‚¹ï¼Œä»¥ä¾¿æ—¥å†ä¸­å®‰æ’æ€æ”¿è¯¾æ¬¡                

                st.success("âœ… å¤§çº²ç”ŸæˆæˆåŠŸï¼")

    if st.session_state.gen_content["syllabus"]:
        st.markdown("---")
        st.container(border=True).markdown(st.session_state.gen_content["syllabus"])
        col1, col2 = st.columns(2)
        col1.download_button("ğŸ’¾ ä¸‹è½½ Word ç‰ˆå¤§çº²", create_docx(st.session_state.gen_content["syllabus"]), file_name=f"{name}_å¤§çº².docx")
        col2.download_button("ğŸ“ ä¸‹è½½æ–‡æœ¬ç‰ˆ (TXT)", st.session_state.gen_content["syllabus"], file_name=f"{name}_å¤§çº².txt")        



# ==================== 1. æ ¸å¿ƒæ¸²æŸ“ä¸è¾…åŠ©å‡½æ•° ====================
# --- è¾…åŠ©å‡½æ•°ï¼šè¯»å–æ¨¡ç‰ˆç»“æ„ ---
def read_local_docx_structure(file_path):
    if not os.path.exists(file_path):
        return "æ¨¡ç‰ˆæ–‡ä»¶ä¸å­˜åœ¨"
    try:
        doc = Document(file_path)
        return "\n".join([p.text for p in doc.paragraphs if "{{" in p.text])
    except:
        return "æ¨¡ç‰ˆè¯»å–å¤±è´¥"

# --- æ ¸å¿ƒå‡½æ•°ï¼šæ¸²æŸ“ Word æ–‡æ¡£ ---
def render_calendar_docx(template_path, data_dict, sig_images=None):
    """
    data_dict: åŒ…å«æ‰€æœ‰æ ‡ç­¾é”®å€¼çš„å­—å…¸
    sig_images: å­—å…¸ï¼Œæ ¼å¼ä¸º {"æ ‡ç­¾å": æ–‡ä»¶æµ}
    """
    try:
        doc = DocxTemplate(template_path)
        
        # 1. é€’å½’æ¸…æ´—æ•°æ®ä¸­çš„ None æˆ– N/A
        def clean_val(v):
            if v is None or str(v).lower() in ["none", "n/a", "æœªæä¾›"]: return ""
            return v

        processed_data = {}
        for k, v in data_dict.items():
            if k == "schedule": # è¿›åº¦è¡¨ç‰¹æ®Šå¤„ç†
                processed_data[k] = [{sk: clean_val(sv) for sk, sv in item.items()} for item in v]
            else:
                processed_data[k] = clean_val(v)

        # 2. æ³¨å…¥ç­¾åå›¾ç‰‡
        if sig_images:
            for key, img_stream in sig_images.items():
                if img_stream:
                    # å°†ä¸Šä¼ çš„å›¾ç‰‡è½¬æ¢ä¸º Word å†…éƒ¨å¯¹è±¡ï¼Œå®½åº¦è®¾ä¸º 30mm
                    processed_data[key] = InlineImage(doc, img_stream, width=Mm(30))
                else:
                    processed_data[key] = ""

        # 3. æ¸²æŸ“å¹¶å¯¼å‡º
        doc.render(processed_data, autoescape=True)
        target_stream = io.BytesIO()
        doc.save(target_stream)
        return target_stream.getvalue()
    except Exception as e:
        st.error(f"æ¸²æŸ“å¤±è´¥: {str(e)}")
        return None


# --- æ•™å¸ˆç«¯ï¼šç¼–æŠ¥é¡µé¢ ---
def render_teacher_view():
    st.markdown("#### ğŸ“ æ•™å¸ˆç«¯ï¼šæ•™å­¦æ—¥å†ç¼–æŠ¥")
    
    # --- 1. åŸºç¡€ä¸è¯¾ç¨‹ä¿¡æ¯ (å…¨é¡¹) ---
    with st.container(border=True):
        st.markdown("##### ğŸ‘¤ 1. åŸºæœ¬ä¿¡æ¯")
     
        c1, c2, c3 = st.columns([1.5, 2, 1.5])
        school_name = c1.text_input("å­¦æ ¡åç§°", key="school_name")
        course_name = c2.text_input("è¯¾ç¨‹åç§°", value=st.session_state.get('course_name', ""))
        class_info = c3.text_input("é€‚ç”¨ä¸“ä¸šåŠå¹´çº§", value=st.session_state.get('major', ""))
        
        t1, t2, t3, t4 = st.columns(4)
        teacher_name = t1.text_input("ä¸»è®²æ•™å¸ˆ", value=st.session_state.get('teacher_name', ""))
        teacher_title = t2.text_input("èŒç§°", value=st.session_state.get('teacher_title', ""))
        academic_year = t3.text_input("å­¦å¹´ (å¦‚ 2025-2026)", value="2025-2026")
        semester = t4.selectbox("å­¦æœŸ", ["1", "2"])

    # --- 2. å­¦æ—¶ä¸æ•™æé…ç½® (å…¨é¡¹) ---
    with st.container(border=True):
        st.markdown("##### ğŸ“š 2. å­¦æ—¶åˆ†é…ä¸æ•™æ")
        h1, h2, h3, h4 = st.columns(4)
        total_hours = h1.number_input("æ€»å­¦æ—¶æ•°", value=int(st.session_state.get('total_hours', 24)))
        term_hours = h2.number_input("æœ¬å­¦æœŸæ€»å­¦æ—¶", value=total_hours)
        total_weeks = h3.number_input("ä¸Šè¯¾å‘¨æ•°", value=12)
        weekly_hours = h4.number_input("å¹³å‡æ¯å‘¨å­¦æ—¶", value=total_hours//total_weeks if total_weeks > 0 else 2)

        d1, d2, d3, d4, d5 = st.columns(5)
        lec_h = d1.number_input("è®²è¯¾å­¦æ—¶", value=total_hours)
        lab_h = d2.number_input("å®éªŒå­¦æ—¶", value=0)
        qui_h = d3.number_input("æµ‹éªŒå­¦æ—¶", value=0)
        ext_h = d4.number_input("è¯¾å¤–å­¦æ—¶", value=0)
        course_nature = d5.text_input("è¯¾ç¨‹æ€§è´¨", value="ä¸“ä¸šå¿…ä¿®")

        st.markdown("---")
        m1, m2, m3, m4 = st.columns([2, 1, 1, 1])
        book_name = m1.text_input("æ•™æåç§°", value=st.session_state.get("textbook_name", ""))
        publisher = m2.text_input("å‡ºç‰ˆç¤¾", value=st.session_state.get("publisher", ""))
        pub_date = m3.text_input("å‡ºç‰ˆæ—¶é—´", value=st.session_state.get('publish_date', ""))
        book_remark = m4.text_input("è·å¥–æƒ…å†µ", value=st.session_state.get('textbook_remark', ""))
        ref_books = st.text_area("å‚è€ƒä¹¦ç›®", value=st.session_state.get("references_text", ""))
        
        k1, k2 = st.columns(2)
        current_val = st.session_state.get('assessment_method', 'è€ƒæŸ¥')
        assess_method = k1.radio("è€ƒæ ¸æ–¹å¼", ["è€ƒè¯•", "è€ƒæŸ¥"], horizontal=True, 
                                 index=0 if "è€ƒè¯•" in current_val else 1)
        grading_formula = k2.text_input("æˆç»©è®¡ç®—æ–¹æ³•", value="æ€»æˆç»©=å¹³æ—¶æˆç»© 30%+è€ƒè¯•æˆç»© 70%")                         


    # --- 3. å¤‡æ³¨ä¸ç­¾å ---
    with st.container(border=True):
        st.markdown("##### ğŸ“ 3. å…¶ä»–ä¿¡æ¯")
        n1, n2, n3 = st.columns(3)
        note_1 = n1.text_input("å¤‡æ³¨1", value="åœ¨æˆè¯¾è¿‡ç¨‹ä¸­ï¼Œå¯èƒ½æ ¹æ®å­¦ç”Ÿæ¥å—æƒ…å†µï¼Œå¾®è°ƒè¯¾ç¨‹è¿›åº¦")
        note_2 = n2.text_input("å¤‡æ³¨2", value="é‡åˆ°å¶å‘æƒ…å†µéœ€è¦è°ƒè¯¾ï¼Œéœ€å±¥è¡Œè°ƒåœè¯¾æ‰‹ç»­")
        note_3 = n3.text_input("å¤‡æ³¨3", value="")
        
        teacher_sig_file = st.file_uploader("âœï¸ ä¸Šä¼ /æ›´æ¢æ‰‹å†™ç­¾å", type=['png', 'jpg'], key="t_sig_up")

    # --- 4. è¿›åº¦è¡¨ç¼–è¾‘ (å«å­¦æ—¶æ‹†åˆ†) ---
    st.divider()
    st.markdown("##### ğŸ—“ï¸ 4. è¿›åº¦å®‰æ’ (å­¦æ—¶ > 2 è‡ªåŠ¨æ‹†åˆ†)")
    syllabus_file = st.file_uploader("é€šè¿‡å¤§çº²æŠ½å–å†…å®¹ (å¯é€‰)", type=['docx', 'pdf'])
    
    # åœ¨ç‚¹å‡»æŒ‰é’®åçš„é€»è¾‘ä¸­
    if st.button("ğŸª„ ä¾æ®å¤§çº²æŠ½å–å¹¶è‡ªåŠ¨æ‹†åˆ†å­¦æ—¶"):
    
        syl_content = ""
        if syllabus_file:
            syl_content = safe_extract_text(syllabus_file)
        else:
            # å°è¯•ä»ä¸Šä¸€é¡µç”Ÿæˆçš„å¤§çº²ä¸­è·å–ï¼Œè‹¥æ— åˆ™ä¸ºç©ºå­—ç¬¦ä¸²
            syl_content = st.session_state.gen_content.get("syllabus") or ""
        
        if not syl_content.strip():
            st.warning("âš ï¸ æœªæ£€æµ‹åˆ°å¤§çº²å†…å®¹ã€‚è¯·å…ˆä¸Šä¼ å¤§çº²æ–‡ä»¶ï¼Œæˆ–åœ¨â€œæ•™å­¦å¤§çº²ç”Ÿæˆâ€é¡µé¢å…ˆç”Ÿæˆå¤§çº²ã€‚")
            return

        with st.spinner("æ­£åœ¨æ·±åº¦è§£æå¤§çº²å¹¶åŒæ­¥å¡«æŠ¥ä¿¡æ¯..."):
            syl_ctx = safe_extract_text(syllabus_file) if syllabus_file else st.session_state.gen_content.get("syllabus", "")
            
            # å®šä¹‰å®Œæ•´æå–æç¤ºè¯
            split_prompt = f"""
            # è§’è‰²
            ä½ æ˜¯ä¸€ä½ç²¾é€š OBE ç†å¿µçš„é«˜æ ¡æ•™åŠ¡ä¸“å®¶ã€‚
            
            # ä»»åŠ¡
            è§£ææä¾›çš„ã€æ•™å­¦å¤§çº²ã€‘ï¼Œæå–æ‰€æœ‰å¡«æŠ¥é¡¹ï¼Œå¹¶ç”Ÿæˆä¸¥æ ¼å¯¹é½è¯¾æ¬¡çš„æ•™å­¦æ—¥å† JSONã€‚
            
            # æ ¸å¿ƒçº¦æŸï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
            1. **æ•°å­¦å¹³è¡¡**ï¼šæ€»å­¦æ—¶ä¸º {total_hours}ï¼Œæ€»å‘¨æ•°ä¸º {total_weeks}ã€‚ç»è®¡ç®—ï¼Œæ¯å‘¨å¿…é¡»ç²¾ç¡®å®‰æ’ ã€{weekly_hours}ã€‘ å­¦æ—¶ã€‚
            2. **å‘¨å­¦æ—¶å®šé¢**ï¼šåœ¨ schedule åˆ—è¡¨ä¸­ï¼ŒåŒä¸€å‘¨(week)å†…æ‰€æœ‰é¡¹çš„ hrs ä¹‹å’Œå¿…é¡»ã€ç»å¯¹ç­‰äºã€‘{weekly_hours}ã€‚
            3. **æ‹†åˆ†é€»è¾‘**ï¼šè‹¥å¤§çº²æŸæ¨¡å—å­¦æ—¶ > {weekly_hours}ï¼Œå¿…é¡»æ‹†åˆ†ä¸ºè¿ç»­çš„ä¸¤å‘¨ï¼ˆæˆ–æ›´å¤šï¼‰ã€‚ä¾‹å¦‚ï¼šæ¨¡å—X(4å­¦æ—¶) -> ç¬¬Nå‘¨(2å­¦æ—¶) + ç¬¬N+1å‘¨(2å­¦æ—¶)ã€‚
            4. **åˆå¹¶é€»è¾‘**ï¼šè‹¥æŸæ¨¡å—å­¦æ—¶ä¸º 1ï¼Œå¿…é¡»ä¸å¤§çº²ä¸‹ä¸€ä¸ªæ¨¡å—åˆå¹¶åœ¨åŒä¸€å‘¨(week)å†…ï¼Œç¡®ä¿è¯¥å‘¨æ€»å­¦æ—¶ä¸º {weekly_hours}ã€‚
            
            # æå–å­—æ®µè¦æ±‚
            è¯·ä»å¤§çº²ä¸­æå–å¹¶è¾“å‡ºä»¥ä¸‹ JSON ç»“æ„ï¼š
            {{
                "base_info": {{
                    "course_name": "ä»å¤§çº²æ ‡é¢˜æˆ–ç¬¬ä¸€è¡¨æå–è¯¾ç¨‹åç§°",
                    "textbook_name": "æ•™æåç§°",
                    "publisher": "å‡ºç‰ˆç¤¾",
                    "publish_date": "å‡ºç‰ˆæ—¶é—´",
                    "textbook_remark": "è·å¥–æƒ…å†µ",
                    "references": "å‚è€ƒä¹¦ç›®å­—ç¬¦ä¸²",
                    "assessment_method": "è€ƒè¯•æˆ–è€ƒæŸ¥",
                    "grading_formula": "æˆç»©è®¡ç®—æ–¹æ³•",
                    "lecture_hours": è®²è¯¾å­¦æ—¶(æ•°å­—),
                    "lab_hours": å®éªŒå­¦æ—¶(æ•°å­—),
                    "quiz_hours": æµ‹éªŒå­¦æ—¶(æ•°å­—),
                    "extra_hours": è¯¾å¤–å­¦æ—¶(æ•°å­—)
                }},

                "schedule": [
                    {{ "week": 1, "sess": 1, "content": "ç« èŠ‚å†…å®¹", "req": "é‡ç‚¹è¦æ±‚", "hrs": æ•°å­—, "method": "æ–¹æ³•", "other": "ä½œä¸š", "obj": "ç›®æ ‡", "source_text": "å¤§çº²åŸæ–‡ç‰‡æ®µ" }}
                ]
            }}
            
            # å‚è€ƒèµ„æ–™
            æ•™å­¦å¤§çº²å†…å®¹ï¼š{syl_ctx[:10000]}
            """
            
            res = ai_generate(split_prompt, engine_id, selected_model)
            try:
                # # 1. è§£æ JSON
                # match = re.search(r'\{.*\}', res, re.DOTALL)
                # full_data = json.loads(match.group(0))
                
                # # 2. è‡ªåŠ¨åˆ·æ–° UI å­—æ®µï¼ˆå°†æå–çš„ä¿¡æ¯å­˜å…¥ session_stateï¼‰
                # bi = full_data.get("base_info", {})
                
                # --- æ ¸å¿ƒä¿®å¤ï¼šè§£å†³ Extra Data æŠ¥é”™ ---
                # è´ªå©ªåŒ¹é…æœ€åä¸€ä¸ªèŠ±æ‹¬å·ï¼Œç¡®ä¿åªæˆªå–æœ€å®Œæ•´çš„ JSON å—
                match = re.search(r'(\{.*\})', res, re.DOTALL)
                if not match:
                    st.error("AI æœªè¿”å›æœ‰æ•ˆçš„ JSON æ ¼å¼")
                    return
                
                json_str = match.group(1).strip()
                full_data = json.loads(json_str)
                bi = full_data.get("base_info", {})  
                st.session_state["textbook_name"] = bi.get("textbook_name", "")
                st.session_state["publisher"] = bi.get("publisher", "")
                st.session_state["publish_date"] = bi.get("publish_date", "")
                st.session_state["textbook_remark"] = bi.get("textbook_remark", "")
                st.session_state["references_text"] = bi.get("references", "")
                st.session_state["assessment_method"] = bi.get("assessment_method", "è€ƒæŸ¥")
                st.session_state["grading_formula"] = bi.get("grading_formula", "")
                
                # 3. è¿›åº¦è¡¨æ•°æ®å¤„ç†
                raw_schedule = full_data.get("schedule", [])
                st.session_state.calendar_data = pd.DataFrame(raw_schedule).fillna("").astype(str).to_dict('records')
                
                st.success("âœ… å¤§çº²ä¿¡æ¯å·²åŒæ­¥åˆ·æ–°è‡³ä¸Šæ–¹è¡¨å•ï¼")
                st.rerun() # å¼ºåˆ¶åˆ·æ–°é¡µé¢ä»¥æ˜¾ç¤ºæ–°æ•°æ®
            except Exception as e:
                st.error(f"è§£æå¹¶åŒæ­¥å¤±è´¥: {str(e)}")

    if st.session_state.calendar_data:
        # éšè— source_text ä»¥ä¿æŒé¡µé¢æ•´æ´ï¼Œä½†ä¿ç•™åœ¨æ•°æ®ä¸­
        st.session_state.calendar_data = st.data_editor(
            pd.DataFrame(st.session_state.calendar_data).astype(str),
            column_config={
                "source_text": None, # éšè—åŸæ–‡ä¾æ®åˆ—ï¼Œä¸æ˜¾ç¤ºä½†ä¿ç•™æ•°æ®
                "content": st.column_config.TextColumn("æ•™å­¦å†…å®¹", width="large"),
                "hrs": st.column_config.NumberColumn("å­¦æ—¶", min_value=1, max_value=4)
            },
            num_rows="dynamic", use_container_width=True
        ).to_dict('records')
        
        
    # --- 5. æäº¤å®¡æ‰¹ (ç»Ÿä¸€å˜é‡åä¸º calendar_final_data) ---
    if st.button("ğŸ“¤ æäº¤æ•™å­¦æ—¥å†å®¡æ‰¹", type="primary", use_container_width=True):
        if not st.session_state.calendar_data:
            st.error("è¿›åº¦è¡¨å†…å®¹ä¸ºç©ºï¼Œæ— æ³•æäº¤ã€‚")
        else:
            ref_list = [line.strip() for line in ref_books.split('\n') if line.strip()]
            # å°è£…ä¸º template_general.docx éœ€è¦çš„æ‰€æœ‰é”® 
            st.session_state.calendar_final_data = {
                "school_name": school_name, "academic_year": academic_year, "semester": semester,
                "course_name": course_name, "class_info": class_info, "teacher_name": teacher_name,
                "teacher_title": teacher_title, "total_hours": total_hours, "term_hours": term_hours,
                "total_weeks": total_weeks, "weekly_hours": weekly_hours, "course_nature": course_nature,
                "lecture_hours": lec_h, "lab_hours": lab_h, "quiz_hours": qui_h, "extra_hours": ext_h,
                "textbook_name": book_name, "publisher": publisher, "publish_date": pub_date,
                "textbook_remark": book_remark, 
                #"references": [ref_books], 
                "assessment_method": assess_method,
                "grading_formula": grading_formula, "schedule": st.session_state.calendar_data,
                "note_1": note_1, "note_2": note_2, "note_3": note_3,
                "sign_date_1": datetime.now().strftime("%Yå¹´ %mæœˆ %dæ—¥"),
                "references": ref_list, # ä¼ å…¥æ‹†åˆ†åçš„åˆ—è¡¨ï¼Œç¡®ä¿æ¨¡æ¿å¯ä»¥å¾ªç¯æ¸²æŸ“
            }
            st.session_state.teacher_sign_img_file = teacher_sig_file
            st.session_state.calendar_status = "Pending_Head"
            st.success("âœ… å·²æäº¤è‡³ç³»ä¸»ä»»å®¡æ‰¹ï¼")
            st.rerun()

def render_approval_view(role):
    st.markdown(f"#### ğŸ›¡ï¸ {'ç³»ä¸»ä»»' if role == 'Head' else 'ä¸»ç®¡é™¢é•¿'}å®¡æ‰¹ç•Œé¢")
    
    # æ ¸å¿ƒå®‰å…¨æ£€æŸ¥ï¼šå¦‚æœæ•°æ®åŒ…ä¸å­˜åœ¨ï¼Œæ˜¾ç¤ºæç¤ºè€ŒéæŠ¥é”™
    data = st.session_state.get("calendar_final_data")
    if not data:
        st.info("ğŸµ ç›®å‰æ²¡æœ‰å¾…å¤„ç†çš„æ•™å­¦æ—¥å†ç”³è¯·ã€‚")
        return

    target_status = "Pending_Head" if role == "Head" else "Pending_Dean"
    if st.session_state.calendar_status == target_status:
        st.info(f"å¾…å¤„ç†ï¼š{data['course_name']} (æ•™å¸ˆï¼š{data['teacher_name']})")
        st.table(pd.DataFrame(data['schedule']).drop(columns=['source_text'], errors='ignore'))
        
        with st.form(f"form_{role}"):
            opinion = st.text_area("å®¡æ‰¹æ„è§", value="åŒæ„ã€‚")
            sig_file = st.file_uploader("ç­¾ç½²æ‰‹å†™ç­¾å", type=['png', 'jpg'])
            c1, c2 = st.columns(2)
            if c1.form_submit_button("âœ… æ‰¹å‡†"):
                st.session_state[f"{role.lower()}_opinion"] = opinion
                st.session_state[f"{role.lower()}_sig_img"] = sig_file
                st.session_state[f"{role.lower()}_date"] = datetime.now().strftime("%Yå¹´ %mæœˆ %dæ—¥")
                st.session_state.calendar_status = "Pending_Dean" if role == "Head" else "Approved"
                st.rerun()
            if c2.form_submit_button("âŒ é€€å›"):
                st.session_state.calendar_status = "Draft"
                st.rerun()
    else:
        st.write("ğŸµ æš‚æ— å¾…åŠäº‹é¡¹ã€‚")

def page_calendar():
    nav_bar(show_back=True)
    st.subheader("ğŸ“… æ•™å­¦æ—¥å†ç¼–æŠ¥ä¸å¤šçº§å®¡æ‰¹")
    
    user_role = st.sidebar.selectbox("åˆ‡æ¢è§’è‰²è§†å›¾", ["æˆè¯¾æ•™å¸ˆ", "ç³»ä¸»ä»»", "ä¸»ç®¡é™¢é•¿"])
    
    if user_role == "æˆè¯¾æ•™å¸ˆ": render_teacher_view()
    elif user_role == "ç³»ä¸»ä»»": render_approval_view("Head")
    else: render_approval_view("Dean")

# --- 7. å®¡æ‰¹è¿‡ç¨‹å®æ—¶æ˜¾ç¤º (æ–°å¢æ¨¡å—) ---
    st.divider()
    st.markdown("##### ğŸš¥ æ•™å­¦æ—¥å†å®¡æ‰¹è¿›åº¦ç›‘æ§")
    
    # å®šä¹‰çŠ¶æ€æ˜ å°„ä¸è¿›åº¦ç™¾åˆ†æ¯”
    status_map = {
        "Draft": {"val": 10, "label": "è‰æ‹Ÿä¸­", "color": "gray"},
        "Pending_Head": {"val": 40, "label": "å¾…æ•™ç ”å®¤ä¸»ä»»å®¡æ‰¹", "color": "blue"},
        "Pending_Dean": {"val": 70, "label": "å¾…å­¦é™¢ä¸»ç®¡é¢†å¯¼å®¡æ‰¹", "color": "orange"},
        "Approved": {"val": 100, "label": "å®¡æ‰¹å·²é€šè¿‡", "color": "green"}
    }
    
    curr_status = st.session_state.get("calendar_status", "Draft")
    progress_info = status_map.get(curr_status, status_map["Draft"])
    
    # æ¸²æŸ“è¿›åº¦æ¡
    st.progress(progress_info["val"])
    
    # æ¸²æŸ“å¯è§†åŒ–èŠ‚ç‚¹
    n1, n2, n3, n4 = st.columns(4)
    nodes = [("Draft", "è‰æ‹Ÿ"), ("Pending_Head", "ç³»ä¸»ä»»å®¡æ ¸"), ("Pending_Dean", "ä¸»ç®¡é™¢é•¿å®¡æ‰¹"), ("Approved", "å®Œæˆå½’æ¡£")]
    for i, (status_key, label) in enumerate(nodes):
        col = [n1, n2, n3, n4][i]
        if status_map[curr_status]["val"] >= status_map[status_key]["val"]:
            col.success(f"â— {label}")
        else:
            col.write(f"â—‹ {label}")

    # å®¡æ‰¹ç»“æœä¸è¯¦ç»†æ„è§æŸ¥çœ‹åŒºåŸŸ
    with st.expander("ğŸ“‹ æŸ¥çœ‹å®¡æ‰¹æ„è§ä¸ç»“æœè¯¦æƒ…", expanded=(curr_status != "Draft")):
        if curr_status == "Draft":
            st.info("ğŸ’¡ å½“å‰å¤„äºè‰æ‹Ÿé˜¶æ®µï¼Œå°šæœªæäº¤å®¡æ‰¹ã€‚")
        else:
            # 1. æ•™ç ”å®¤ä¸»ä»»å®¡æ‰¹ä¿¡æ¯
            st.markdown("**ã€æ•™ç ”å®¤ä¸»ä»»å®¡æ‰¹ã€‘**")
            head_op = st.session_state.get("head_opinion", "ç­‰å¾…å¤„ç†...")
            st.write(f"> å®¡æ‰¹æ„è§ï¼š{head_op}")
            if "head_date" in st.session_state:
                st.caption(f"å®¡æ‰¹æ—¶é—´ï¼š{st.session_state.head_date}")
            if st.session_state.get("head_sign_img"):
                st.image(st.session_state.head_sign_img, width=120, caption="ç³»ä¸»ä»»ç­¾å")
            
            st.divider()
            
            # 2. å­¦é™¢é¢†å¯¼å®¡æ‰¹ä¿¡æ¯
            st.markdown("**ã€å­¦é™¢ä¸»ç®¡é¢†å¯¼å®¡æ‰¹ã€‘**")
            dean_op = st.session_state.get("dean_opinion", "ç­‰å¾…å¤„ç†...")
            st.write(f"> å®¡æ‰¹æ„è§ï¼š{dean_op}")
            if "dean_date" in st.session_state:
                st.caption(f"å®¡æ‰¹æ—¶é—´ï¼š{st.session_state.dean_date}")
            if st.session_state.get("dean_sign_img"):
                st.image(st.session_state.dean_sign_img, width=120, caption="é™¢é•¿ç­¾å")

    # --- ä¸‹è½½åŒºåŸŸ ---
    if curr_status == "Approved":
        st.balloons()
        final_data = st.session_state.calendar_final_data
        # è¡¥å…¨å®¡æ‰¹æ„è§ 
        final_data.update({
            "head_opinion": st.session_state.get("head_opinion", ""),
            "sign_date_2": st.session_state.get("head_date", ""),
            "dean_opinion": st.session_state.get("dean_opinion", ""),
            "sign_date_3": st.session_state.get("dean_date", "")
        })
        sig_map = {
            "teacher_sign_img": st.session_state.get("teacher_sign_img_file"),
            "head_sign_img": st.session_state.get("head_sig_img"),
            "dean_sign_img": st.session_state.get("dean_sig_img")
        }


        # æ ¸å¿ƒä¿®å¤ï¼šç›´æ¥ä»å·²æäº¤çš„æ•°æ®åŒ…é‡Œè¯»å­¦æ ¡å
        submitted_school = final_data.get("school_name", "").strip()
        
        # ä½¿ç”¨ if-elif-else ç»“æ„æ›´æ¸…æ™°
        if submitted_school == "è¾½å®çŸ³æ²¹åŒ–å·¥å¤§å­¦":
            target_tpl = "template_lnpu.docx"
        else:
            target_tpl = "template_general.docx"

        # æ‰§è¡Œå¡«å……
        doc_bytes = render_calendar_docx(target_tpl, final_data, sig_map)

        if doc_bytes:
            st.download_button("ğŸ“¥ ä¸‹è½½å®Œæ•´å®¡æ‰¹ç‰ˆ (.docx)", data=doc_bytes, file_name="æ•™å­¦æ—¥å†_å·²å®¡æ‰¹.docx")
  
def page_program():
    nav_bar(show_back=True)
    st.subheader("ğŸ“‹ ä¸“ä¸šäººæ‰åŸ¹å…»æ–¹æ¡ˆç”Ÿæˆ")
    with st.form("program_form"):
        major = st.text_input("ä¸“ä¸šåç§°", value="ææ–™æˆå‹åŠæ§åˆ¶å·¥ç¨‹")
        pos = st.text_area("ä¸“ä¸šç‰¹è‰²", value="æœåŠ¡çŸ³æ²¹åŒ–å·¥è¡Œä¸šï¼Œèšç„¦ç„Šæ¥æˆå‹ä¸æ— æŸæ£€æµ‹")
        if st.form_submit_button("ç”Ÿæˆäººæ‰åŸ¹å…»æ–¹æ¡ˆ"):
            prompt = f"æ’°å†™{major}ä¸“ä¸š2024çº§åŸ¹å…»æ–¹æ¡ˆã€‚å«åŸ¹å…»ç›®æ ‡ã€12é¡¹æ¯•ä¸šè¦æ±‚ã€ç‰¹è‰²å®šä½({pos})ã€æ ¸å¿ƒè¯¾ç¨‹ã€‚ä¸“ä¸šä¸¥è°¨ã€‚"
            with st.spinner("æ­£åœ¨æ„å»ºæ–¹æ¡ˆ..."):
                st.session_state.generated_program = ai_generate(prompt, engine_id, selected_model)

    if st.session_state.generated_program:
        st.markdown("---")
        st.container(border=True).markdown(st.session_state.gen_content["program"])
        st.download_button("ğŸ’¾ ä¸‹è½½ Word ç‰ˆåŸ¹å…»æ–¹æ¡ˆ", create_docx(st.session_state.gen_content["program"]), file_name="åŸ¹å…»æ–¹æ¡ˆ.docx")

def page_grading():
    nav_bar(show_back=True)
    st.subheader("ğŸ“ æ™ºèƒ½è¯•å·æ‰¹é˜…ä¸è¯„ä»·")
    c1, c2 = st.columns(2)
    with c1:
        q_file = st.file_uploader("1. ä¸Šä¼ è¯•é¢˜ (PDF/Word)", type=["pdf", "docx"], key="q")
        q_txt = extract_text_from_file(q_file) if q_file else ""
    with c2:
        s_file = st.file_uploader("2. ä¸Šä¼ æ ‡å‡†ç­”æ¡ˆ (PDF/Word)", type=["pdf", "docx"], key="s")
        s_txt = extract_text_from_file(s_file) if s_file else ""

    st.divider()
    papers = st.file_uploader("3. æ‰¹é‡ä¸Šä¼ å­¦ç”Ÿå·çº¸ (å›¾ç‰‡/PDF)", type=["jpg", "png", "pdf"], accept_multiple_files=True)

    for idx, paper in enumerate(papers or []):
        with st.container(border=True):
            st.write(f"**å­¦ç”Ÿ {idx+1}:** {paper.name}")
            s_name = st.text_input("å§“å", value=f"å­¦ç”Ÿ_{idx+1}", key=f"sn_{idx}")
            
            ocr_text = ""
            if paper.type == "application/pdf":
                imgs = render_pdf_images(paper)
                for i, img in enumerate(imgs):
                    st.image(img, width=350)
                    with st.expander("ğŸ” æŸ¥çœ‹é«˜æ¸…å¤§å›¾"): st.image(img, use_container_width=True)
                    with st.spinner("è¯†åˆ«ä¸­..."): ocr_text += ai_ocr(img, engine_id, selected_model) + "\n"
            else:
                img_data = paper.read()
                st.image(img_data, width=350)
                with st.expander("ğŸ” æŸ¥çœ‹é«˜æ¸…å¤§å›¾"): st.image(img_data, use_container_width=True)
                with st.spinner("è¯†åˆ«ä¸­..."): ocr_text = ai_ocr(img_data, engine_id, selected_model)
            
            final_ans = st.text_area("è¯†åˆ«ç»“æœæ ¡å¯¹", value=ocr_text, key=f"ocr_{idx}", height=150)
            
            if st.button(f"ğŸš€ {engine_id} è‡ªåŠ¨æ‰¹æ”¹", key=f"go_{idx}"):
                with st.spinner("æ­£åœ¨è¯„åˆ†..."):
                    p = f"é¢˜ç›®ï¼š{q_txt}\nç­”æ¡ˆï¼š{s_txt}\nå­¦ç”Ÿï¼š{final_ans}\nè¯·è¯„åˆ†(æ»¡åˆ†100)å¹¶ç»™å‡ºæ‰¹æ³¨ã€‚æ ¼å¼ï¼š\nåˆ†æ•°ï¼š[æ•°å­—]\næ‰¹æ³¨ï¼š[è§£æ]"
                    res = ai_generate(p, engine_id, selected_model)
                    st.markdown(res)
                    score = int(re.search(r"åˆ†æ•°[ï¼š:]\s*(\d+)", res).group(1)) if re.search(r"åˆ†æ•°[ï¼š:]\s*(\d+)", res) else 0
                    st.session_state.score_records.append({"å­¦ç”Ÿ": s_name, "åˆ†æ•°": score, "è¯„ä»·": res})

def page_analysis():
    nav_bar(show_back=True)
    st.subheader("ğŸ“ˆ æˆç»©ä¸åˆ†ææŠ¥å‘Š")
    if not st.session_state.score_records:
        st.warning("å½“å‰æ— æ‰¹æ”¹è®°å½•")
        return
    st.dataframe(st.session_state.score_records, use_container_width=True)
    scores = [r["åˆ†æ•°"] for r in st.session_state.score_records]
    col1, col2 = st.columns(2)
    with col1:
        st.metric("å¹³å‡åˆ†", f"{np.mean(scores):.1f}")
        fig, ax = plt.subplots(figsize=(8, 4))
        ax.hist(scores, bins=range(0, 110, 10), color='#4F8BF9', edgecolor='white')
        st.pyplot(fig)
    with col2:
        st.download_button("å¯¼å‡ºæˆç»©è®°å½• (CSV)", str(st.session_state.score_records), "scores.csv")

# --- 7. è·¯ç”±é€»è¾‘ ---
# -----------------------------
# Router (fix NameError)
# -----------------------------
route = {
    "é¦–é¡µ": lambda: page_home(),
    "å¤§çº²": lambda: page_syllabus(),
    "æ—¥å†": lambda: page_calendar(),
    "æ–¹æ¡ˆ": lambda: page_program(),    
    "åŸºåº§": lambda: page_base(),
    "æ¨¡æ¿": lambda: page_template_tagger(),
    "æ‰¹å·": lambda: page_grading(),
    "åˆ†æ": lambda: page_analysis(),
    "è®¾ç½®": lambda: page_settings(),  # âœ… å»¶è¿Ÿåˆ°ç‚¹å‡»æ—¶æ‰è§£æåå­—
}

current = st.query_params.get("page", "é¦–é¡µ")
route.get(current, lambda: page_home())()
