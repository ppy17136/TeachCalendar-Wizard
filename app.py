# app.py
# TeachCalendar Wizard / Teaching Agent Suite - Single-file Streamlit app (deployable)
# Focus:
# 1) "åŸºåº§"ï¼šåŸ¹å…»æ–¹æ¡ˆ PDF æŠ½å– 1-11 + é™„è¡¨(7-10) è‡ªåŠ¨æŠ½å– & å¤šé¡µåˆå¹¶
# 2) "æ¨¡æ¿"ï¼šæŠŠä¸Šä¼ çš„ Word èŒƒæœ¬è‡ªåŠ¨æ”¹æˆâ€œå¸¦æ ‡ç­¾çš„æ¨¡æ¿â€ï¼Œå¯ä¸‹è½½ï¼ˆå…ˆä¸å¡«å……ï¼‰
# 3) è·¯ç”±/é¡µé¢å‡½æ•°é½å…¨ï¼šé¿å… NameErrorï¼›Streamlit keys è§„æ•´ï¼šé¿å… DuplicateElementKey / ValueAssignmentNotAllowedError
#
# Dependencies (requirements.txt):
# streamlit, pandas, pdfplumber, python-docx, numpy, matplotlib, pillow
#
from __future__ import annotations

import base64
import datetime as dt
import hashlib
import io
import json
import re
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import pandas as pd
import streamlit as st
import streamlit.components.v1 as components

# Optional imports (fail-safe)
try:
    import pdfplumber
except Exception as _e:
    pdfplumber = None

try:
    from docx import Document
except Exception:
    Document = None


# -----------------------------
# Utils
# -----------------------------
APP_TITLE = "TeachCalendar Wizard"
APP_VERSION = "v0.8.3"

def _now_str() -> str:
    return time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())

def _short_id(s: str) -> str:
    return hashlib.md5(s.encode("utf-8")).hexdigest()[:10]

def _safe_text(x: Any) -> str:
    if x is None:
        return ""
    return str(x).replace("\u00a0", " ").strip()

def _compact_lines(s: str) -> str:
    s = (s or "").replace("\u00a0", " ")
    s = re.sub(r"[ \t]+", " ", s)
    s = re.sub(r"\n{3,}", "\n\n", s)
    return s.strip()

def _join_pages(pages_text: List[str]) -> str:
    return _compact_lines("\n\n".join([t or "" for t in pages_text]))

def _jsonable(obj: Any) -> Any:
    """Make payload JSON serializable (DataFrame/bytes/datetime/numpy)."""
    # DataFrame
    if isinstance(obj, pd.DataFrame):
        df = obj.copy().fillna("")
        return {"__type__": "dataframe", "columns": [str(c) for c in df.columns], "data": df.astype(str).values.tolist()}
    # bytes
    if isinstance(obj, (bytes, bytearray)):
        return {"__type__": "bytes_base64", "data": base64.b64encode(bytes(obj)).decode("ascii")}
    # datetime/date
    if isinstance(obj, (dt.datetime, dt.date)):
        return obj.isoformat()
    # Path
    if isinstance(obj, Path):
        return str(obj)
    # list/tuple/set
    if isinstance(obj, (list, tuple, set)):
        return [_jsonable(x) for x in obj]
    # dict
    if isinstance(obj, dict):
        return {str(k): _jsonable(v) for k, v in obj.items()}
    # numpy scalars/arrays
    try:
        import numpy as np
        if isinstance(obj, (np.integer, np.floating, np.bool_)):
            return obj.item()
        if isinstance(obj, np.ndarray):
            return obj.tolist()
    except Exception:
        pass
    # fallback
    try:
        json.dumps(obj)
        return obj
    except Exception:
        return str(obj)


# -----------------------------
# State / Projects
# -----------------------------
@dataclass
class Project:
    project_id: str
    name: str
    updated_at: str

def _init_state():
    if "projects" not in st.session_state:
        pid = _short_id(_now_str())
        st.session_state.projects = [Project(project_id=pid, name=f"é»˜è®¤é¡¹ç›®-{time.strftime('%Y%m%d-%H%M')}", updated_at=_now_str())]
        st.session_state.active_project_id = pid

    if "project_data" not in st.session_state:
        # project_id -> payload dict
        st.session_state.project_data = {}

    if "logo_bytes" not in st.session_state:
        st.session_state.logo_bytes = None

    if "template_tag_maps" not in st.session_state:
        # project_id -> {"tags":..., "meta":...}
        st.session_state.template_tag_maps = {}


# -----------------------------
# Sidebar
# -----------------------------
def ui_sidebar_brand():
    with st.sidebar:
        col1, col2 = st.columns([1, 4])
        with col1:
            if st.session_state.logo_bytes:
                st.image(st.session_state.logo_bytes, width=44)
            else:
                svg = """
                <div style="width:44px;height:44px;border-radius:50%;
                            background:#2f6fed;display:flex;align-items:center;justify-content:center;
                            color:white;font-weight:800;font-family:Arial;">
                  TC
                </div>
                """
                components.html(svg, height=50)
        with col2:
            st.markdown(f"**{APP_TITLE}**")
            st.caption(f"{APP_VERSION} Â· åŸºåº§æŠ½å– + æ¨¡æ¿æ‰“æ ‡ç­¾")

        up = st.file_uploader("ä¸Šä¼  Logoï¼ˆå¯é€‰ï¼Œpng/jpgï¼‰", type=["png", "jpg", "jpeg"], key="logo_uploader")
        if up is not None:
            st.session_state.logo_bytes = up.getvalue()


def ui_project_sidebar() -> Project:
    ui_sidebar_brand()
    with st.sidebar:
        st.divider()
        st.markdown("### é¡¹ç›®")
        labels = {p.project_id: f"{p.name} ({p.project_id})" for p in st.session_state.projects}
        ids = list(labels.keys())
        idx = ids.index(st.session_state.active_project_id) if st.session_state.active_project_id in ids else 0
        pid = st.selectbox("é€‰æ‹©é¡¹ç›®", options=ids, format_func=lambda x: labels[x], index=idx, key="project_select")
        st.session_state.active_project_id = pid
        return {p.project_id: p for p in st.session_state.projects}[pid]


def _render_top_header(project: Project):
    html = f"""
    <div style="border:1px solid #e7eefc; background:#f6f9ff; padding:16px 18px; border-radius:14px;">
      <div style="font-weight:900; font-size:26px;">æ•™å­¦æ–‡ä»¶å·¥ä½œå°</div>
      <div style="color:#666; margin-top:4px; font-size:13px;">
        é¡¹ç›®ï¼š <b>{project.name}</b>ï¼ˆ{project.project_id}ï¼‰ Â· æœ€åæ›´æ–°ï¼š {project.updated_at}
      </div>
    </div>
    """
    st.markdown(html, unsafe_allow_html=True)


# -----------------------------
# PDF -> Base (1-11) + Appendix tables (7-10) merging
# -----------------------------
_SECTION_PATTERNS: List[Tuple[str, List[str]]] = [
    ("1", [r"^\s*(ä¸€|1)[ã€\.\s]*åŸ¹å…»ç›®æ ‡\b"]),
    ("2", [r"^\s*(äºŒ|2)[ã€\.\s]*æ¯•ä¸šè¦æ±‚\b"]),
    ("3", [r"^\s*(ä¸‰|3)[ã€\.\s]*ä¸“ä¸šå®šä½ä¸ç‰¹è‰²\b"]),
    ("4", [r"^\s*(å››|4)[ã€\.\s]*ä¸»å¹²å­¦ç§‘\b", r"^\s*(å››|4)[ã€\.\s]*ä¸»å¹²å­¦ç§‘.*æ ¸å¿ƒè¯¾ç¨‹", r"^\s*(å››|4)[ã€\.\s]*ä¸»å¹²å­¦ç§‘.*å®è·µ"]),
    ("5", [r"^\s*(äº”|5)[ã€\.\s]*æ ‡å‡†å­¦åˆ¶\b", r"^\s*(äº”|5)[ã€\.\s]*æ ‡å‡†å­¦åˆ¶ä¸æˆäºˆå­¦ä½\b"]),
    ("6", [r"^\s*(å…­|6)[ã€\.\s]*æ¯•ä¸šæ¡ä»¶\b"]),
    ("7", [r"^\s*(ä¸ƒ|7)[ã€\.\s]*ä¸“ä¸šæ•™å­¦è®¡åˆ’è¡¨\b"]),
    ("8", [r"^\s*(å…«|8)[ã€\.\s]*å­¦åˆ†ç»Ÿè®¡è¡¨\b"]),
    ("9", [r"^\s*(ä¹|9)[ã€\.\s]*æ•™å­¦è¿›ç¨‹è¡¨\b"]),
    ("10", [r"^\s*(å|10)[ã€\.\s]*è¯¾ç¨‹è®¾ç½®å¯¹æ¯•ä¸šè¦æ±‚æ”¯æ’‘å…³ç³»è¡¨\b"]),
    ("11", [r"^\s*(åä¸€|11)[ã€\.\s]*è¯¾ç¨‹è®¾ç½®é€»è¾‘æ€ç»´å¯¼å›¾\b"]),
]

def _read_pdf_pages_text(pdf_bytes: bytes) -> List[str]:
    if pdfplumber is None:
        return ["[é”™è¯¯] æœªå®‰è£… pdfplumberï¼Œæ— æ³•è§£æ PDFã€‚"]
    pages = []
    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
        for p in pdf.pages:
            txt = p.extract_text() or ""
            pages.append(_compact_lines(txt))
    return pages

def _find_heading_positions(full_text: str) -> List[Tuple[str, int]]:
    hits: List[Tuple[str, int]] = []
    # Use multiline anchors; search each pattern
    for sec_id, pats in _SECTION_PATTERNS:
        pos = None
        for pat in pats:
            m = re.search(pat, full_text, flags=re.MULTILINE)
            if m:
                pos = m.start()
                break
        if pos is not None:
            hits.append((sec_id, pos))
    hits.sort(key=lambda x: x[1])
    return hits

def _build_section_spans(full_text: str) -> Dict[str, Tuple[int, int]]:
    hits = _find_heading_positions(full_text)
    spans: Dict[str, Tuple[int, int]] = {}
    for i, (sec_id, start) in enumerate(hits):
        end = hits[i + 1][1] if i + 1 < len(hits) else len(full_text)
        spans[sec_id] = (start, end)
    return spans

def _strip_heading_line(chunk: str) -> str:
    # remove first heading line
    chunk = re.sub(r"^\s*(ä¸€|äºŒ|ä¸‰|å››|äº”|å…­|ä¸ƒ|å…«|ä¹|å|åä¸€|\d{1,2})[ã€\.\s]*[^\n]{0,40}\n", "", chunk)
    return _compact_lines(chunk)

def _extract_section_text(full_text: str, spans: Dict[str, Tuple[int, int]], sec_id: str) -> str:
    if sec_id not in spans:
        return ""
    s, e = spans[sec_id]
    return _strip_heading_line(full_text[s:e])

def _valid_table_settings_lines() -> dict:
    # Stable-ish settings for pdfplumber tables
    return dict(
        vertical_strategy="lines",
        horizontal_strategy="lines",
        snap_tolerance=3,
        join_tolerance=3,
        edge_min_length=3,
        intersection_tolerance=3,
        text_tolerance=3,
    )

def _extract_tables_with_meta(pdf_bytes: bytes, page_idx_list: List[int]) -> List[Tuple[int, int, List[List[str]]]]:
    """Return list of (page_idx, table_idx_on_page, table_rows)."""
    if pdfplumber is None:
        return []
    out: List[Tuple[int, int, List[List[str]]]] = []
    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
        for idx in page_idx_list:
            if idx < 0 or idx >= len(pdf.pages):
                continue
            page = pdf.pages[idx]
            tables: List[List[List[str]]] = []
            try:
                tables = page.extract_tables(table_settings=_valid_table_settings_lines()) or []
            except TypeError:
                tables = page.extract_tables() or []
            except Exception:
                try:
                    tables = page.extract_tables() or []
                except Exception:
                    tables = []

            for ti, t in enumerate(tables):
                norm = [[_safe_text(c) for c in row] for row in (t or [])]
                if norm:
                    out.append((idx, ti, norm))
    return out

def _dedup_cols(cols: List[str]) -> List[str]:
    seen = {}
    out = []
    for c in cols:
        c0 = (c or "").strip() or "åˆ—"
        if c0 not in seen:
            seen[c0] = 1
            out.append(c0)
        else:
            seen[c0] += 1
            out.append(f"{c0}_{seen[c0]}")
    return out

def _clean_df(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty:
        return pd.DataFrame()
    df = df.copy()
    df.replace({None: ""}, inplace=True)
    df = df.applymap(lambda x: "" if str(x).strip().lower() == "nan" else str(x).strip())

    # drop all-empty rows/cols
    df = df.loc[~df.apply(lambda r: all((str(x).strip() == "") for x in r), axis=1)]
    df = df.loc[:, ~df.apply(lambda c: all((str(x).strip() == "") for x in c), axis=0)]
    return df.reset_index(drop=True)

def _table_to_df(table_rows: List[List[str]]) -> pd.DataFrame:
    rows = [r for r in (table_rows or []) if any(_safe_text(x) for x in r)]
    if not rows:
        return pd.DataFrame()

    max_cols = max(len(r) for r in rows)
    rows = [r + [""] * (max_cols - len(r)) for r in rows]

    # header-like?
    header = rows[0]
    header_join = " ".join(header)
    header_like = any(k in header_join for k in ["è¯¾ç¨‹", "å­¦åˆ†", "å‘¨æ¬¡", "æŒ‡æ ‡", "æ”¯æ’‘", "åˆè®¡", "è¯¾ç¨‹ç¼–ç ", "è¯¾ç¨‹åç§°", "å­¦æ—¶"])
    if header_like:
        cols = _dedup_cols([c if c else f"åˆ—{i+1}" for i, c in enumerate(header)])
        df = pd.DataFrame(rows[1:], columns=cols)
    else:
        df = pd.DataFrame(rows, columns=[f"åˆ—{i+1}" for i in range(max_cols)])

    df = _clean_df(df)

    # remove repeated header rows inside body
    if not df.empty:
        col_text = [str(c).strip() for c in df.columns]
        def is_repeated_header(row: pd.Series) -> bool:
            vals = [str(x).strip() for x in row.tolist()]
            # if many cells match column names
            matches = sum(1 for a, b in zip(vals, col_text) if a and b and a == b)
            return matches >= max(2, len(col_text)//2)
        df = df.loc[~df.apply(is_repeated_header, axis=1)].reset_index(drop=True)

    return df

def _table_signature_text(df: pd.DataFrame) -> str:
    if df is None or df.empty:
        return ""
    head = " ".join([str(c) for c in df.columns.tolist()])
    top_rows = []
    for i in range(min(3, len(df))):
        top_rows.append(" ".join([str(x) for x in df.iloc[i].tolist()]))
    return (head + " " + " ".join(top_rows)).lower()

def _classify_table(df: pd.DataFrame) -> Tuple[str, int]:
    """Return (section_id, score) where section_id in {"7","8","9","10"}."""
    s = _table_signature_text(df)

    score7 = sum(3 for k in ["è¯¾ç¨‹ç¼–ç ", "è¯¾ç¨‹ä»£ç ", "è¯¾ç¨‹åç§°", "å­¦åˆ†", "æ€»å­¦æ—¶", "è€ƒæ ¸", "å¼€è¯¾"] if k in s)
    score8 = sum(3 for k in ["å­¦åˆ†ç»Ÿè®¡", "å¿…ä¿®", "é€‰ä¿®", "é€šè¯†", "ä¸“ä¸š", "å®è·µ", "åˆè®¡", "å°è®¡"] if k in s)
    score9 = sum(3 for k in ["å‘¨æ¬¡", "æ•™å­¦å†…å®¹", "è¿›åº¦", "ç« èŠ‚", "å­¦æ—¶", "å®éªŒ"] if k in s)
    score10 = sum(3 for k in ["æ¯•ä¸šè¦æ±‚", "æŒ‡æ ‡ç‚¹", "æ”¯æ’‘", "è¾¾æˆ", "å¯¹åº”", "è¯¾ç¨‹è®¾ç½®å¯¹æ¯•ä¸šè¦æ±‚"] if k in s)

    best = max([("7", score7), ("8", score8), ("9", score9), ("10", score10)], key=lambda x: x[1])
    return best if best[1] >= 6 else ("", 0)

def _merge_dfs(parts: List[pd.DataFrame]) -> pd.DataFrame:
    """Merge multi-page table parts to one df (best-effort)."""
    parts = [p for p in parts if p is not None and not p.empty]
    if not parts:
        return pd.DataFrame()
    # choose base columns by most frequent / longest
    base = max(parts, key=lambda d: d.shape[1])
    base_cols = [str(c) for c in base.columns]

    aligned: List[pd.DataFrame] = []
    for df in parts:
        d = df.copy()
        # if same col count but different names, keep base cols
        if d.shape[1] == len(base_cols):
            d.columns = base_cols
        else:
            # align by padding/truncation
            cols = [f"åˆ—{i+1}" for i in range(d.shape[1])]
            d.columns = cols
            # pad
            if d.shape[1] < len(base_cols):
                for i in range(d.shape[1], len(base_cols)):
                    d[f"åˆ—{i+1}"] = ""
            d = d.iloc[:, :len(base_cols)]
            d.columns = base_cols
        aligned.append(d)

    merged = pd.concat(aligned, axis=0, ignore_index=True)
    merged = _clean_df(merged)

    # remove duplicate consecutive rows (common in page breaks)
    if not merged.empty:
        merged = merged.loc[~merged.duplicated()].reset_index(drop=True)
    return merged

def extract_appendix_tables_best_effort(pdf_bytes: bytes, pages_text: List[str]) -> Tuple[Dict[str, pd.DataFrame], Dict[str, Any]]:
    """
    ä» PDF æœ«å°¾é¡µé¢æŠ½å–è¡¨æ ¼ï¼Œè‡ªåŠ¨åˆ†ç±»åˆ° 7-10ï¼Œå¹¶å¯¹â€œè·¨å¤šé¡µâ€çš„åŒä¸€é™„è¡¨è¿›è¡Œåˆå¹¶ã€‚
    è¯´æ˜ï¼š
      - å…ˆç”¨å…³é”®è¯æ‰“åˆ†åˆ†ç±»ï¼›
      - å†ç”¨â€œç»­æ¥â€å¯å‘å¼ï¼šå¦‚æœæŸé¡µè¡¨æ ¼æœªå‘½ä¸­å…³é”®è¯ï¼Œä½†ç´§è·Ÿåœ¨å·²è¯†åˆ«é™„è¡¨åã€ä¸”åˆ—ç»“æ„ç›¸è¿‘ï¼Œåˆ™è§†ä¸ºåŒä¸€é™„è¡¨çš„åç»­é¡µã€‚
    Returns:
      tables_map: {"7":df, "8":df, "9":df, "10":df}
      debug_meta
    """
    n = len(pages_text)
    tail_pages = list(range(max(0, n - 18), n))  # last 18 pages (æ›´ç¨³ä¸€ç‚¹)
    raw = _extract_tables_with_meta(pdf_bytes, tail_pages)
    raw_sorted = sorted(raw, key=lambda x: (x[0], x[1]))

    parts_with_meta: Dict[str, List[Tuple[int, int, pd.DataFrame]]] = {"7": [], "8": [], "9": [], "10": []}
    classified_log = []

    # â€œç»­æ¥â€ä¸Šä¸‹æ–‡
    active_sec: Optional[str] = None
    active_until_page: int = -1
    active_cols: Optional[List[str]] = None
    active_ncols: int = -1

    def _col_similarity(cols_a: List[str], cols_b: List[str]) -> float:
        if not cols_a or not cols_b:
            return 0.0
        a = set([c.strip().lower() for c in cols_a if c.strip()])
        b = set([c.strip().lower() for c in cols_b if c.strip()])
        if not a or not b:
            return 0.0
        return len(a & b) / max(1, len(a | b))

    for page_idx, ti, rows in raw_sorted:
        df = _table_to_df(rows)
        if df is None or df.empty:
            continue
        if df.shape[0] < 2 and df.shape[1] < 3:
            continue

        sec, score = _classify_table(df)
        cols = [str(c) for c in df.columns]
        ncols = df.shape[1]

        used_as_continuation = False
        if not sec and active_sec and page_idx <= active_until_page:
            # ç»­æ¥åˆ¤æ®ï¼šåˆ—æ•°ç›¸è¿‘ æˆ– åˆ—åç›¸ä¼¼
            sim = _col_similarity(cols, active_cols or [])
            if abs(ncols - (active_ncols if active_ncols > 0 else ncols)) <= 1 or sim >= 0.35:
                sec = active_sec
                score = 1  # ç»­æ¥åˆ†ï¼ˆä½äºå…³é”®è¯å‘½ä¸­ï¼‰
                used_as_continuation = True

        if sec:
            parts_with_meta[sec].append((page_idx, ti, df))
            classified_log.append({
                "page": page_idx, "table": ti, "sec": sec, "score": score,
                "shape": list(df.shape), "continuation": used_as_continuation
            })

            # æ›´æ–°ä¸Šä¸‹æ–‡ï¼šå…³é”®è¯å‘½ä¸­æ—¶æ›´å¼ºï¼›ç»­æ¥æ—¶ä¹Ÿå»¶é•¿ä¸€ç‚¹ç‚¹
            if not used_as_continuation:
                active_sec = sec
                active_until_page = page_idx + 3
                active_cols = cols
                active_ncols = ncols
            else:
                # continuationï¼šè½»å¾®å»¶é•¿
                active_until_page = max(active_until_page, page_idx + 2)

        else:
            classified_log.append({
                "page": page_idx, "table": ti, "sec": "", "score": 0,
                "shape": list(df.shape), "continuation": False
            })

    merged_map: Dict[str, pd.DataFrame] = {}
    for sec in ["7", "8", "9", "10"]:
        sec_parts = sorted(parts_with_meta[sec], key=lambda x: (x[0], x[1]))
        merged_map[sec] = _merge_dfs([x[2] for x in sec_parts])

    debug = {
        "tail_pages": tail_pages,
        "raw_tables_count": len(raw),
        "classified_log": classified_log[:120],
        "merged_shapes": {k: (list(v.shape) if isinstance(v, pd.DataFrame) else None) for k, v in merged_map.items()},
    }
    return merged_map, debug



def base_plan_from_pdf(pdf_bytes: bytes) -> Dict[str, Any]:
    pages = _read_pdf_pages_text(pdf_bytes)
    full = _join_pages(pages)
    spans = _build_section_spans(full)

    sections: Dict[str, str] = {}
    for sec_id, _ in _SECTION_PATTERNS:
        sections[sec_id] = _extract_section_text(full, spans, sec_id)

    # If 7-11 empty in main body, put hint
    for sec_id in ["7", "8", "9", "10", "11"]:
        if not sections.get(sec_id, "").strip():
            sections[sec_id] = f"{sec_id}ï¼šæ­£æ–‡å¯èƒ½ä»…æœ‰æ ‡é¢˜ï¼›è¯·å°è¯•ä» PDF æœ«å°¾é™„è¡¨è‡ªåŠ¨æŠ½å–ã€‚"

    tables, debug = extract_appendix_tables_best_effort(pdf_bytes, pages)
    return {"pages": pages, "full_text": full, "sections": sections, "tables": tables, "debug": debug}


# -----------------------------
# Word Template Tagger (create tagged template for docxtpl)
# -----------------------------
def _set_cell_text_keep_style(cell, new_text: str):
    # Clear cell content but keep cell formatting
    # python-docx doesn't have a direct clear; replace first paragraph then clear others
    paras = cell.paragraphs
    if not paras:
        cell.text = new_text
        return
    # set first paragraph
    p0 = paras[0]
    # clear runs
    for r in list(p0.runs):
        r.text = ""
    if p0.runs:
        p0.runs[0].text = new_text
    else:
        p0.add_run(new_text)
    # clear remaining paragraphs
    for p in paras[1:]:
        for r in list(p.runs):
            r.text = ""

def _set_paragraph_text_keep_style(p, new_text: str):
    # Keep paragraph style; keep first run formatting as much as possible
    runs = list(p.runs)
    if not runs:
        p.add_run(new_text)
        return
    # overwrite first run text
    runs[0].text = new_text
    # clear other runs
    for r in runs[1:]:
        r.text = ""

def tag_docx_to_template(docx_bytes: bytes, mode: str = "all") -> Tuple[bytes, Dict[str, str], Dict[str, Any]]:
    """
    Replace non-empty texts with {{tags}}.
    mode:
      - "tables": only tag table cells
      - "paragraphs": only tag paragraphs
      - "all": both
    Return: (template_bytes, tag_map[tag]=original_text, meta)
    """
    if Document is None:
        raise RuntimeError("æœªå®‰è£… python-docxï¼Œæ— æ³•å¤„ç† Wordã€‚")

    doc = Document(io.BytesIO(docx_bytes))
    tag_map: Dict[str, str] = {}
    counters = {"p": 0, "t": 0, "h": 0, "f": 0}

    def make_tag(prefix: str) -> str:
        counters[prefix] += 1
        return f"{prefix}{counters[prefix]:03d}"

    def should_skip(text: str) -> bool:
        t = _safe_text(text)
        if not t:
            return True
        # already has docxtpl tag
        if "{{" in t and "}}" in t:
            return True
        # skip pure page number / short punctuation
        if len(t) <= 1:
            return True
        if re.fullmatch(r"[\d\-\./]+", t):
            return True
        return False

    # paragraphs in body
    if mode in ("all", "paragraphs"):
        for p in doc.paragraphs:
            raw = p.text
            if should_skip(raw):
                continue
            tag = make_tag("p")
            tag_text = "{{" + tag + "}}"
            tag_map[tag] = raw
            _set_paragraph_text_keep_style(p, tag_text)

    # tables in body
    if mode in ("all", "tables"):
        for tb_i, table in enumerate(doc.tables, start=1):
            for r_i, row in enumerate(table.rows, start=1):
                for c_i, cell in enumerate(row.cells, start=1):
                    raw = cell.text
                    if should_skip(raw):
                        continue
                    tag = make_tag("t")
                    tag_text = "{{" + tag + "}}"
                    tag_map[tag] = raw
                    _set_cell_text_keep_style(cell, tag_text)

    # headers/footers
    def tag_header_footer(container, prefix: str):
        for p in container.paragraphs:
            raw = p.text
            if should_skip(raw):
                continue
            tag = make_tag(prefix)
            tag_text = "{{" + tag + "}}"
            tag_map[tag] = raw
            _set_paragraph_text_keep_style(p, tag_text)
        for table in container.tables:
            for row in table.rows:
                for cell in row.cells:
                    raw = cell.text
                    if should_skip(raw):
                        continue
                    tag = make_tag(prefix)
                    tag_text = "{{" + tag + "}}"
                    tag_map[tag] = raw
                    _set_cell_text_keep_style(cell, tag_text)

    if mode == "all":
        for sec in doc.sections:
            tag_header_footer(sec.header, "h")
            tag_header_footer(sec.footer, "f")

    out = io.BytesIO()
    doc.save(out)
    meta = {"tag_count": len(tag_map), "mode": mode, "counters": counters}
    return out.getvalue(), tag_map, meta


# -----------------------------
# Pages
# -----------------------------
def nav_bar():
    # simple nav via query params
    with st.sidebar:
        st.divider()
        st.markdown("### å¯¼èˆª")
        pages = ["é¦–é¡µ", "å¤§çº²", "æ—¥å†", "æ–¹æ¡ˆ", "åŸºåº§", "æ¨¡æ¿", "æ‰¹å·", "åˆ†æ", "è®¾ç½®"]
        current = st.query_params.get("page", "é¦–é¡µ")
        choice = st.radio("é¡µé¢", pages, index=pages.index(current) if current in pages else 0, key="nav_radio")
        st.query_params["page"] = choice

def page_home():
    st.subheader("ğŸ  é¦–é¡µ")
    st.write("è¿™é‡Œæ˜¯æ•™å­¦æ–‡ä»¶å·¥ä½œå°çš„é¦–é¡µã€‚å»ºè®®ä½¿ç”¨å·¦ä¾§å¯¼èˆªè¿›å…¥ã€åŸºåº§ã€‘æˆ–ã€æ¨¡æ¿ã€‘ã€‚")
    st.info("å¦‚æœä½ åªæƒ³å…ˆéªŒè¯â€œWord æ¨¡æ¿æ‰“æ ‡ç­¾â€æ˜¯å¦æˆåŠŸï¼šè¿›å…¥ã€æ¨¡æ¿ã€‘â†’ ä¸Šä¼  docx â†’ ä¸€é”®ç”Ÿæˆæ ‡ç­¾æ¨¡æ¿å¹¶ä¸‹è½½ã€‚")

def page_syllabus():
    st.subheader("ğŸ“˜ å¤§çº²")
    st.info("å ä½é¡µï¼šä½ å¯ä»¥æŠŠâ€œè¯¾ç¨‹å¤§çº²ç”Ÿæˆ/æ ¡å¯¹â€æ¨¡å—æ”¾åœ¨è¿™é‡Œã€‚")

def page_calendar():
    st.subheader("ğŸ“… æ•™å­¦æ—¥å†")
    st.info("å ä½é¡µï¼šä½ å¯ä»¥æŠŠâ€œæ•™å­¦æ—¥å†å¡«å……/å¯¼å‡ºâ€æ¨¡å—æ”¾åœ¨è¿™é‡Œï¼ˆç¨åæ¥å…¥ DocxTemplate æ¸²æŸ“ï¼‰ã€‚")

def page_program():
    st.subheader("ğŸ§© åŸ¹å…»æ–¹æ¡ˆ")
    st.info("å ä½é¡µï¼šä½ å¯ä»¥æŠŠâ€œåŸ¹å…»æ–¹æ¡ˆç®¡ç†/å¯¹æ¯”/å®¡æ ¸â€æ¨¡å—æ”¾åœ¨è¿™é‡Œã€‚")

def page_grading():
    st.subheader("ğŸ“ æ‰¹å·")
    st.info("å ä½é¡µï¼šä½ å¯ä»¥æŠŠâ€œè¯•å·ä¸Šä¼ /è¯†åˆ«/æ‰¹é˜…/è¯„ä»·â€æ¨¡å—æ”¾åœ¨è¿™é‡Œã€‚")

def page_analysis():
    st.subheader("ğŸ“Š åˆ†æ")
    st.info("å ä½é¡µï¼šä½ å¯ä»¥æŠŠâ€œæ•°æ®ç»Ÿè®¡/è´¨é‡åˆ†æ/è¾¾æˆåº¦åˆ†æâ€æ¨¡å—æ”¾åœ¨è¿™é‡Œã€‚")

def page_settings():
    st.subheader("âš™ï¸ è®¾ç½®")
    st.write("è¿™é‡Œæ”¾ä¸€äº›å¼€å…³ã€é»˜è®¤å‚æ•°ç­‰ã€‚")
    st.checkbox("å¯ç”¨ PDF é™„è¡¨æŠ½å–ï¼ˆ7-10ï¼‰", value=True, key="cfg_enable_appendix")
    st.checkbox("æ¨¡æ¿æ‰“æ ‡ç­¾æ—¶åŒæ—¶å¤„ç†é¡µçœ‰é¡µè„š", value=True, key="cfg_tag_header_footer")

def page_base():
    """Alias for compatibility: route may call page_base()."""
    return page_base_plan()

def page_base_plan():
    st.subheader("ğŸ§± åŸ¹å…»æ–¹æ¡ˆåŸºåº§ï¼ˆå…¨é‡å†…å®¹åº“ï¼‰")
    st.caption("ä¸Šä¼ åŸ¹å…»æ–¹æ¡ˆ PDF â†’ æŠ½å–å¡«å…… 1â€“11 â†’ å¹¶å°è¯•ä»æœ«å°¾é™„è¡¨è‡ªåŠ¨æŠ½è¡¨å¡«å…… 7â€“10ï¼ˆæ”¯æŒå¤šé¡µåˆå¹¶ï¼‰ã€‚")

    project: Project = st.session_state.__active_project  # set in main
    left, right = st.columns([1, 1.4], gap="large")

    with left:
        if pdfplumber is None:
            st.error("å½“å‰ç¯å¢ƒæœªå®‰è£… pdfplumberï¼Œæ— æ³•è§£æ PDFã€‚è¯·åœ¨ requirements.txt æ·»åŠ  pdfplumberã€‚")
            return

        pdf = st.file_uploader("ä¸Šä¼ åŸ¹å…»æ–¹æ¡ˆ PDFï¼ˆ.pdfï¼‰", type=["pdf"], key=f"pdf_{project.project_id}")

        if st.button("æŠ½å–å¹¶å†™å…¥åŸºåº§", use_container_width=True, type="primary", key=f"extract_btn_{project.project_id}"):
            if not pdf:
                st.warning("è¯·å…ˆä¸Šä¼  PDFã€‚")
            else:
                pdf_bytes = pdf.getvalue()
                payload = base_plan_from_pdf(pdf_bytes)
                st.session_state.project_data[project.project_id] = payload

                # update project timestamp
                for i, p in enumerate(st.session_state.projects):
                    if p.project_id == project.project_id:
                        st.session_state.projects[i] = Project(project_id=p.project_id, name=p.name, updated_at=_now_str())
                        st.session_state.__active_project = st.session_state.projects[i]
                        break

                st.success("å·²æŠ½å–å¹¶å†™å…¥åŸºåº§ã€‚å³ä¾§å·²è”åŠ¨å¡«å……ã€‚")
                st.rerun()

        payload = st.session_state.project_data.get(project.project_id)

        if payload:
            # Download JSON (fixed)
            json_payload = _jsonable(payload)
            st.download_button(
                label="ä¸‹è½½åŸºåº§ JSON",
                data=json.dumps(json_payload, ensure_ascii=False, indent=2).encode("utf-8"),
                file_name=f"base_{project.project_id}.json",
                mime="application/json",
                use_container_width=True,
                key=f"dl_{project.project_id}",
            )

        st.divider()
        if payload:
            # quality checks: section text mostly for 1-6, tables for 7-10
            miss = []
            for k in [str(i) for i in range(1, 7)]:
                if not (payload.get("sections", {}).get(k, "") or "").strip():
                    miss.append(k)
            if miss:
                st.warning(f"æ­£æ–‡æŠ½å–ç¼ºå°‘æ ç›®ï¼š{miss}ï¼ˆå¯èƒ½ PDF æ ‡é¢˜æ ¼å¼ä¸ä¸€è‡´ï¼‰")
            else:
                st.success("æ­£æ–‡ 1â€“6 å·²æŠ½å–ï¼ˆå»ºè®®äººå·¥æ‰«è¯»ï¼‰ã€‚")

            assigned = payload.get("debug", {}).get("merged_shapes", {})
            st.write("é™„è¡¨æŠ½å–ç»“æœï¼ˆåˆå¹¶åå½¢çŠ¶ï¼‰:", assigned)

        with st.expander("è°ƒè¯•ï¼šåˆ†é¡µåŸæ–‡ (raw_pages_text)"):
            if payload:
                st.write(payload.get("pages", []))
            else:
                st.info("å…ˆæŠ½å–åå¯è§ã€‚")

        with st.expander("è°ƒè¯•ï¼šé™„è¡¨æŠ½å–ä¿¡æ¯ (appendix_debug)"):
            if payload:
                st.json(payload.get("debug", {}))
            else:
                st.info("å…ˆæŠ½å–åå¯è§ã€‚")

    with right:
        st.markdown("#### åŸ¹å…»æ–¹æ¡ˆå†…å®¹ï¼ˆæŒ‰æ ç›®å±•ç¤ºï¼Œå¯ç¼–è¾‘ï¼‰")

        payload = st.session_state.project_data.get(project.project_id)
        if not payload:
            st.info("è¯·å…ˆåœ¨å·¦ä¾§ä¸Šä¼  PDF å¹¶ç‚¹å‡»â€œæŠ½å–å¹¶å†™å…¥åŸºåº§â€ã€‚")
            return

        sections = payload.get("sections", {})
        tables = payload.get("tables", {}) or {}

        toc = [
            ("1", "åŸ¹å…»ç›®æ ‡"),
            ("2", "æ¯•ä¸šè¦æ±‚"),
            ("3", "ä¸“ä¸šå®šä½ä¸ç‰¹è‰²"),
            ("4", "ä¸»å¹²å­¦ç§‘/æ ¸å¿ƒè¯¾ç¨‹/å®è·µç¯èŠ‚"),
            ("5", "æ ‡å‡†å­¦åˆ¶ä¸æˆäºˆå­¦ä½"),
            ("6", "æ¯•ä¸šæ¡ä»¶"),
            ("7", "ä¸“ä¸šæ•™å­¦è®¡åˆ’è¡¨ï¼ˆé™„è¡¨1ï¼‰"),
            ("8", "å­¦åˆ†ç»Ÿè®¡è¡¨ï¼ˆé™„è¡¨2ï¼‰"),
            ("9", "æ•™å­¦è¿›ç¨‹è¡¨ï¼ˆé™„è¡¨3ï¼‰"),
            ("10", "æ”¯æ’‘å…³ç³»è¡¨ï¼ˆé™„è¡¨4ï¼‰"),
            ("11", "é€»è¾‘æ€ç»´å¯¼å›¾ï¼ˆé™„è¡¨5ï¼‰"),
        ]
        title_map = dict(toc)

        sec_pick = st.radio(
            "æ ç›®",
            options=[x[0] for x in toc],
            format_func=lambda x: title_map[x],
            horizontal=True,
            key=f"sec_radio_{project.project_id}",
        )

        st.markdown(f"##### {sec_pick}ã€{title_map[sec_pick]}")

        txt = sections.get(sec_pick, "") or ""

        # extra safety truncate: 6 should not contain 7+
        if sec_pick == "6":
            m = re.search(r"^\s*(ä¸ƒ|7)[ã€\.\s]*ä¸“ä¸šæ•™å­¦è®¡åˆ’è¡¨\b", txt, flags=re.MULTILINE)
            if m:
                txt = _compact_lines(txt[:m.start()])

        st.text_area(
            f"{sec_pick} æ–‡æœ¬æŠ½å–ç»“æœï¼ˆå¯ç¼–è¾‘ï¼‰",
            value=txt,
            height=220,
            key=f"sec_text_{project.project_id}_{sec_pick}",
        )

        # Table editors for 7-10
        if sec_pick in ["7", "8", "9", "10"]:
            st.markdown("###### è¡¨æ ¼åŒºï¼ˆå¯ç¼–è¾‘ï¼Œè¡Œå¯å¢åˆ ï¼‰")
            df0 = tables.get(sec_pick)
            if df0 is None or (isinstance(df0, pd.DataFrame) and df0.empty):
                st.warning("æœªè‡ªåŠ¨æŠ½å–åˆ°è¯¥é™„è¡¨ï¼ˆå¯èƒ½ PDF è¡¨æ ¼æ˜¯å›¾ç‰‡æˆ–çº¿æ¡ä¸è§„åˆ™ï¼‰ã€‚å¯å…ˆæ‰‹å·¥è¡¥å…¨ï¼Œæˆ–ç¨åæ¥å…¥ OCRã€‚")
                df0 = pd.DataFrame()

            editor_key = f"tbl_editor_{project.project_id}_{sec_pick}"
            edited = st.data_editor(df0, num_rows="dynamic", use_container_width=True, key=editor_key)
            # store value separately (avoid ValueAssignmentNotAllowedError)
            st.session_state[f"{editor_key}__value"] = edited

        if sec_pick == "11":
            st.info("é€»è¾‘æ€ç»´å¯¼å›¾ï¼ˆé™„è¡¨5ï¼‰é€šå¸¸æ˜¯å›¾ç‰‡/æµç¨‹å›¾ï¼›å¦‚éœ€è‡ªåŠ¨æŠ½å–ï¼Œå¯åç»­åŠ â€œæœ«é¡µå›¾ç‰‡æå–â€æˆ–æ‰‹åŠ¨ä¸Šä¼ å›¾ç‰‡ã€‚")


def page_template_tagger():
    st.subheader("ğŸ·ï¸ Word æ¨¡æ¿æ‰“æ ‡ç­¾ï¼ˆå…ˆä¸å¡«å……ï¼‰")
    st.caption("ä¸Šä¼  docx â†’ è‡ªåŠ¨æŠŠæ®µè½/è¡¨æ ¼ä¸­çš„éç©ºæ–‡å­—æ›¿æ¢ä¸º {{tag}} â†’ ä¸‹è½½æ¨¡æ¿ + ä¸‹è½½æ ‡ç­¾æ˜ å°„è¡¨ã€‚")

    if Document is None:
        st.error("å½“å‰ç¯å¢ƒæœªå®‰è£… python-docxï¼Œæ— æ³•å¤„ç† Wordã€‚è¯·åœ¨ requirements.txt æ·»åŠ  python-docxã€‚")
        return

    project: Project = st.session_state.__active_project
    col1, col2 = st.columns([1, 1])

    with col1:
        mode = st.selectbox("æ‰“æ ‡ç­¾èŒƒå›´", options=["all", "tables", "paragraphs"],
                            format_func=lambda x: {"all":"æ®µè½+è¡¨æ ¼+é¡µçœ‰é¡µè„š","tables":"ä»…è¡¨æ ¼","paragraphs":"ä»…æ®µè½"}[x],
                            key=f"tag_mode_{project.project_id}")
        docx = st.file_uploader("ä¸Šä¼  Word èŒƒæœ¬ï¼ˆ.docxï¼‰", type=["docx"], key=f"tmpl_{project.project_id}")

        run_btn = st.button("ä¸€é”®ç”Ÿæˆæ ‡ç­¾æ¨¡æ¿", type="primary", use_container_width=True, key=f"tag_btn_{project.project_id}")

    with col2:
        st.markdown("**è¯´æ˜**")
        st.write("- æ¯ä¸ªè¢«æ›¿æ¢çš„ä½ç½®ä¼šç”Ÿæˆä¸€ä¸ªå”¯ä¸€æ ‡ç­¾ï¼Œå¦‚ `{{p001}}` / `{{t012}}`ã€‚")
        st.write("- æ ‡ç­¾æ˜ å°„è¡¨ä¼šè®°å½• `tag -> åŸå§‹æ–‡å­—`ï¼Œä¾¿äºä½ åç»­å¯¹æ¥ AI å¡«å……ã€‚")
        st.write("- ç›®å‰ç­–ç•¥æ˜¯â€œå°½é‡ä¿æŒæ ¼å¼â€ï¼Œä½†å¤æ‚ run çº§åˆ«æ ·å¼å¯èƒ½ä¼šè¢«ç®€åŒ–ã€‚")

    if run_btn:
        if not docx:
            st.warning("è¯·å…ˆä¸Šä¼  docxã€‚")
        else:
            with st.spinner("æ­£åœ¨ç”Ÿæˆæ ‡ç­¾æ¨¡æ¿..."):
                tpl_bytes, tag_map, meta = tag_docx_to_template(docx.getvalue(), mode=mode)
                st.session_state.template_tag_maps[project.project_id] = {"tag_map": tag_map, "meta": meta, "template_bytes": tpl_bytes, "source_name": docx.name, "ts": _now_str()}
            st.success(f"âœ… å·²ç”Ÿæˆæ ‡ç­¾æ¨¡æ¿ï¼šå…± {len(tag_map)} ä¸ªæ ‡ç­¾ã€‚")

    result = st.session_state.template_tag_maps.get(project.project_id)
    if result:
        st.divider()
        st.markdown("#### ä¸‹è½½")
        base_name = Path(result.get("source_name", "template.docx")).stem
        st.download_button(
            "ä¸‹è½½ï¼šæ ‡ç­¾æ¨¡æ¿ï¼ˆdocxï¼‰",
            data=result["template_bytes"],
            file_name=f"{base_name}_tagged.docx",
            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
            use_container_width=True,
            key=f"dl_tagged_{project.project_id}",
        )
        st.download_button(
            "ä¸‹è½½ï¼šæ ‡ç­¾æ˜ å°„è¡¨ï¼ˆjsonï¼‰",
            data=json.dumps(_jsonable({"tag_map": result["tag_map"], "meta": result["meta"]}), ensure_ascii=False, indent=2).encode("utf-8"),
            file_name=f"{base_name}_tag_map.json",
            mime="application/json",
            use_container_width=True,
            key=f"dl_tagmap_{project.project_id}",
        )

        with st.expander("é¢„è§ˆï¼šå‰ 30 ä¸ªæ ‡ç­¾æ˜ å°„"):
            tag_map = result["tag_map"]
            items = list(tag_map.items())[:30]
            st.dataframe(pd.DataFrame(items, columns=["tag", "original_text"]), use_container_width=True)


# -----------------------------
# Router
# -----------------------------
def _route_call(page: str):
    # Keep both names to avoid NameError even if old route dict uses page_base/page_base_plan.
    route = {
        "é¦–é¡µ": page_home,
        "å¤§çº²": page_syllabus,
        "æ—¥å†": page_calendar,
        "æ–¹æ¡ˆ": page_program,
        "åŸºåº§": page_base,                 # compatibility
        "æ¨¡æ¿": page_template_tagger,
        "æ‰¹å·": page_grading,
        "åˆ†æ": page_analysis,
        "è®¾ç½®": page_settings,
    }
    fn = route.get(page, page_home)
    fn()


def main():
    st.set_page_config(page_title=APP_TITLE, page_icon="ğŸ§ ", layout="wide")
    _init_state()

    prj = ui_project_sidebar()
    st.session_state.__active_project = prj  # internal convenience

    nav_bar()
    _render_top_header(prj)

    # main page based on query param
    current = st.query_params.get("page", "é¦–é¡µ")
    _route_call(current)


if __name__ == "__main__":
    main()
